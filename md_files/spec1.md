3D人体形状復元システム 実装仕様書（ポーズ情報入力対応 改訂版）

概要と使用技術

本システムは正面および背面のカラー画像から被写体の体型パラメータβ（STARモデル）を高精度に推定する3D人体形状復元システムです。従来手法であるDongらのDual-view法線マップベース復元法を踏襲しつつ、SMPLの代替としてSTARモデルを採用しています。さらに本改訂版では、入力画像から得られるポーズ情報（2D関節キーポイント）や人物セグメンテーション情報も活用し、形状推定の精度と安定性を向上させています。具体的には、Meta社のSapiensモデルから得られる高精度な法線マップ・深度マップ・2D関節位置・部位セグメンテーションを入力特徴として統合し、STARモデルの形状空間上で被写体の体型を推定します。Sapiensは2Dポーズ推定・部位セグメンテーション・深度推定・法線予測を統合した大規模モデル（最大10億パラメータ級）で、300万以上の人物画像で自己教師あり学習されており、高解像度(1K)入力でも優れた精度と汎化性能を示します。本システムでは推論時にSapiensのTorchScript版（軽量化・推論専用モデル）を使用し、RGB画像から表面法線マップ・深度マップ・人物マスク・2D関節位置・部位セグメンテーションを取得します。モデル規模はResNet18ベースで数百万パラメータ程度に抑えていますが、推論時はサーバーGPU上で実行することで高精度を確保しています（現状はリアルタイム性より精度を優先）。学習データ不足は大規模な合成データ生成で補い、STRAPSにならったプロキシ表現（法線・深度マップ、シルエット等）による学習で効率化しています。主要な評価指標として、推定した3Dモデルから算出したウエスト・ヒップ周囲長の誤差を用い、被験者の身体寸法に対する精度を評価します。また、ユーザから提供可能な補助情報（身長や体重、性別など）を前処理の深度スケール補正やネットワークの追加入力に組み込み、推定精度向上に役立てています（詳細は後述）。

以下では、使用ライブラリ・モデル、推論パイプライン、学習パイプライン、および実装上の詳細設定について説明します。

使用ライブラリ・モデル
	•	PyTorch (>= 2.x): 深層学習フレームワーク。CNNの構築・学習や自動微分、GPU活用を担います。本システムのモデルおよび合成データ生成・レンダリング処理はPyTorch上で実装しています。Mixed Precision Training (AMP) に対応しており、高解像度データの学習でもメモリ効率よく実行可能です。
	•	PyTorch3D: PyTorch用3Dレンダリング／3D操作ライブラリ。合成データ生成時にSTARメッシュから法線マップ・深度マップ・セグメンテーションマップをレンダリングするのに使用します。また微分可能レンダリングにより、学習時・推論時の最適化工程でモデル出力と画像との整合性評価が可能です。
	•	STARモデル実装: MPI提供のSTARモデルのPyTorch実装（公式GitHubのahmedosman/STAR等）を使用します。男女別およびジェンダーニュートラル版の.npzモデルファイルをロードし、推論・学習時に微分可能な形で利用します。STARは300次元の形状主成分空間を持ちますが、本実装では用途に合わせ必要な次元数のみを使用しています。PyTorch上でSTARの計算（シェイプブレンドシェイプ適用＋ポーズスキニング）をカスタムレイヤー化し、GPU上で効率的に動作させます。STARはSMPL互換の人体モデルですが、STAR特有のスパース補正項があるため、基本的には公式実装に沿って組み込んでいます（SMPL用コードからの流用は差異があるため見送りました）。
	•	Sapiens (Meta AI): Meta社が公開している人間中心視覚モデルです。人物画像から表面法線・奥行き（深度）・2Dポーズ（関節位置）・部位セグメンテーションなどを高精度に推定できます。ViTベースのEncoder-Decoderアーキテクチャで大量の人物画像で事前訓練されており、高精度と汎化性能を示します。本システムでは推論時にSapiensのTorchScript版（推論専用に蒸留した小型モデル）を使用し、入力RGB画像から高品質な法線マップ・深度マップ・人物マスク・2D関節キーポイント・部位セグメンテーションマップを取得します。Sapiensは約10億パラメータの巨大モデルですが、蒸留した“小型版”によりGPU上で1枚あたり約0.3秒の推論を実現しました（社内検証での実測値）。TorchScript化によりPythonなしでもC++環境で動作可能なため、将来的なモバイルデバイス搭載にも備えています。
	•	OpenCV / PIL: 画像の前処理やデータ拡張（リサイズ、ブラー、ノイズ付加）に使用します。PyTorchのデータローダ（torchvision.transforms）と組み合わせ、リアルタイムに画像変換を行います。
	•	Hardware: NVIDIA Tesla V100/A100クラスのGPUサーバー上での動作を想定しています。推論時にはSapiensモデルやCNN推定器をGPUで実行し、リアルタイムに近い応答速度を得ます。学習時もすべてGPU上で計算を行い、バッチ並列計算で効率化します。推奨スペックはGPUメモリ16GB以上、CPUはマルチスレッド処理対応のものです。カメラ内部パラメータ（焦点距離等）は事前に較正して既知とします（例えば50mm相当の視野角約30°と仮定）。これにより身長と画像上のピクセル寸法からおおよその被写体までの距離を見積もり、前処理の深度スケール補正や後述のカメラ距離補正で利用します。

以上が主要な技術スタックです。次に、推論パイプラインの実装詳細を順に説明します。

推論パイプライン詳細

本節では、概要フローに沿って推論処理各ステップを具体的に説明します。
	1.	前処理ステップ: Sapiensによる特徴マップ生成
画像入力とSapiens推論: ユーザ入力のRGB正面・背面画像をPILで読み込み、所定の解像度にリサイズします（デフォルト512px。一時的に1024pxにも対応可）。次にSapiensモデルに入力し、表面法線マップ・深度マップ・人物マスク・2D関節キーポイント・部位セグメンテーションを推論します。実装上はTorchScript化されたSapiensを用い、例えば sapiens_model(torch_tensor) により複数の出力を同時取得します（単一モデルがマルチタスク出力）。出力はPyTorchのTensor形式（例: depth_tensor:形状1×1×H×W、normal_tensor:形状1×3×H×W、pose_tensor:形状1×K×3（関節K個の2D座標＋信頼度）、segmentation_tensor:形状1×H×W（画素毎の部位ラベル））です。そこで各出力をNumpy配列等に変換して画像や配列として処理できるようにします。
	•	人物マスク: Sapiensのセグメンテーション出力から人物領域マスク（H×Wの2値画像）を取得します。Sapiensは人物領域外にもノイズ的な値を出力する可能性があるため、後続の法線・深度・ポーズ処理ではこのマスクで背景を無効化します。
	•	法線データの前処理: OpenCVで法線画像を読み込み（H×W×3配列）、float32に変換してから/255.0で[0,1]に正規化します。さらに normal = normal * 2 - 1 を画素毎に適用し、値域を（ほぼ）[-1,1]にリスケーリングします。念のため各画素について$\sqrt{n_x^2 + n_y^2 + n_z^2}$を計算して単位法線ベクトルに正規化します（Sapiensの出力はほぼ正規化済みですが、微小なズレを補正）。背景ピクセル（人物マスク外）については、対応するnormal値を(0,0,0)に設定します（ネットワーク側でこれら無効ベクトルが学習時に無視されるよう工夫しています）。
	•	深度データの前処理: OpenCVで深度マップ（H×Wの1ch画像）を読み込み、float32に変換します。人物マスク外の深度値は0に設定します。一方、人物領域内の深度値はカメラからの相対距離を表すため、ここでスケール補正を行います。具体的には、正面画像について人物マスク内の画素のy座標の最大値・最小値を取得して人物の画面内高さ$H_{px}$を算出します。ユーザから入力された実身長$H_{\text{user}}$（単位m）がある場合、それを被写体の真の身長$H_{\text{real}}$とみなし、スケーリング係数 $S = H_{\text{real}} / H_{px}$ を計算し、深度マップ全体に乗じます（$depth = depth * S$）。これにより、画像内での人物の高さが$H_{\text{real}}$に対応するスケールで深度が調整されます（例えば、画素高さ$H_{px}=900$px・実身長$H_{\text{real}}=1.8$mなら$S \approx 0.002$となり、深度値を0.002倍して人物の頭頂～足底までの深度差が約1.8に揃うイメージです）。背面画像についても同様に処理します（同一人物なので$H_{\text{real}}$は共通ですが、画像内高さ$H_{px}$は微妙に異なる場合があるため別途計算して適用します）。この深度正規化により、ネットワークに与える深度マップは全被写体で統一スケールとなり、異なるカメラ距離の影響は主に後段で推定する$T_z$（奥行き方向並進）に委ねられます。実装上、ユーザが$H_{\text{user}}$を提供しないケースは想定していませんが、もし未入力ならデフォルト値$H_{\text{real}}=1.7$mとして同様の計算を行います。
	•	ポーズデータの前処理: Sapiensから出力された2D関節キーポイント（各関節の画像座標および信頼度）を取得します。これをネットワーク入力で扱える形式に変換します。本システムでは、各関節をガウシアンヒートマップとして画像チャネルにエンコードする手法を採用しました。これは、関節座標を直接MLPに入力する方法よりも空間的文脈を保ったままCNN特徴と融合できるため、ベストプラクティスと判断したためです。具体的には、正面画像について検出されたK個の関節について、同じ解像度H×Wの空白画像をKチャネル分用意し、それぞれのチャネルに該当関節の2D位置を中心とする局所的な2次元ガウシアン分布を描画します（例えばσ=5px程度の標準偏差を持つピーク1.0のガウシアンをブラー描画）。この結果、各チャネルは特定関節の存在確率マップ（ヒートマップ）になります。Sapiensの信頼度スコアも利用可能ですが、本実装では信頼度が一定以上の関節のみを描画し、低信頼度の関節は出力しない処理を行いました（大半のケースで正面・背面とも主要関節は高信頼度で取得されています）。背面画像についても同様にKチャネルの関節ヒートマップ群を生成します。なお、人物マスク外の領域にはヒートマップを描かないようにし、ガウシアン分布は人物領域内に限定します。こうして作成した関節ヒートマップはfloat32に変換し、[0,1]範囲に正規化して扱います。
	•	セグメンテーションデータの前処理: Sapiensの部位セグメンテーション出力（各画素が頭部・胴体・腕・脚などのクラスIDを持つラベルマップ）を取得します。これをネットワーク入力用にワンホットの部位マスクチャネルに変換します。例えば、部位カテゴリを背景を含めてC種類定義し（背景、頭、胴、上腕（左/右）、下腕（左/右）、上脚（左/右）、下脚（左/右）、足（左/右）等で約14クラスを想定）、画素ごとに該当クラスのチャネルを1、それ以外を0とするCチャネルのバイナリマップに変換します。こうすることで、CNN入力として各部位領域を明示した形で利用できます（例えばチャネルごとに「ここは頭部」「ここは胴体」等の情報が与えられる）。背景領域はどの部位チャネルにも属さない（ゼロ）状態となります。人物マスクで背景を除去しているため、背景画素は全チャネル0で表現されます。部位セグメンテーションを入力特徴として加えることで、ネットワークが身体のどの部位の画素かを認識しやすくなり、法線や輪郭形状だけでは曖昧な情報（例えば腕と胴が重なっている箇所の形状など）の解釈が容易になることを狙っています。
ネットワーク入力形式への変換: 上記処理により、正面用には法線3ch + 深度1ch + 関節ヒートマップKch + 部位セグメンテーションCchのデータ、背面用にも同様のデータが得られます。これらをそれぞれ torch.from_numpy でPyTorchのTensor（形状: $(3+1+K+C) \times H \times W$）に変換し、unsqueeze(0)でバッチ次元（サイズ1）を追加します。こうして正面入力テンソル front_input（サイズ: $1 \times (3+1+K+C) \times H \times W$）と背面入力テンソル back_input を用意します（例えばK=17関節、C=14部位の場合、各は$1\times 35\times H \times W$程度のテンソルとなります）。なお、ユーザ提供の身長・体重・性別などの属性情報があればここで取得し、後述するネットワークへの付加入力として準備します。数値属性は正規化やエンコードを行った上でTensor化し（例: 身長Hは1.7m基準比$H_{\text{user}}/1.7$、体重Wは標準体重との差分比、性別はMale=0/Female=1のワンホット等）、attr_inputテンソルとして保持します。これら属性テンソルはモデル内部で画像特徴と結合して利用されます（詳細は次項）。
	2.	形状推定ネットワーク: モデル構造と特徴統合
ネットワーク構造: 形状推定のニューラルネットワークはPyTorchで実装したtorch.nn.Moduleで、正面・背面の2視点入力をそれぞれ処理し、その特徴を統合してβおよびカメラ相対位置$T$を推定します。基本構造はResNet18ベースのCNNで、入力チャネル数を増やしたエンコーダを2視点で重み共有して用います。具体的には以下のようになります:

self.resnet = torchvision.models.resnet18(pretrained=True)
# 入力層の畳み込みをカスタマイズ（入力チャンネル数を増やす）
self.resnet.conv1 = nn.Conv2d(3+1+K+C, 64, kernel_size=7, stride=2, padding=3, bias=False)
# conv1の重みは、学習済みResNetのRGB用フィルタを流用し、
# 新規チャネル(K+C+…-3)についてはRGBフィルタ平均値で初期化
old_weight = pretrained_resnet.conv1.weight  # (64,3,7,7)
new_weight = torch.zeros(64, 3+1+K+C, 7, 7)
new_weight[:, :3, :, :] = old_weight  # RGB部分をコピー
mean_rgb = old_weight.mean(dim=1, keepdim=True)  # RGBフィルタ平均
new_weight[:, 3:(3+1+K+C), :, :] = mean_rgb  # 4チャネル目以降を平均値で初期化
self.resnet.conv1.weight = nn.Parameter(new_weight)
# ResNetの全結合層は使用せず、出力512次元の特徴ベクトルを抽出する
self.resnet.fc = nn.Identity()

ResNet18の畳み込み層・プーリング層構造は基本的にImageNet学習済みのまま変更しません。上記のように、入力層のみチャンネル数を$(3+1+K+C)$に合わせて置換し、学習済みモデルの重みを一部初期値として利用します（新規チャネル分はRGBフィルタの平均値や微小乱数で初期化）。これにより、法線・深度・ヒートマップ・セグメンテーションといった複数モダリティの画像情報を一括して受け取り、特徴抽出できるCNNエンコーダが構築されます。正面画像と背面画像をそれぞれこのResNetエンコーダ（重み共有）に通し、512次元のグローバル特徴ベクトルを抽出します。実行時のフローは以下の通りです:

def forward(self, front_input, back_input, attr_input=None):
    feat_f = self.resnet(front_input)   # 正面: 入力(3+1+K+Cch) -> 出力特徴512次元
    feat_b = self.resnet(back_input)    # 背面: 同一ResNet（重み共有）で特徴抽出
    feat_cat = torch.cat([feat_f, feat_b], dim=1)  # 結合して1024次元に
    # ユーザ属性があれば特徴に連結
    if attr_input is not None:
        feat_cat = torch.cat([feat_cat, attr_input], dim=1)  # attrは適宜拡張・正規化済ベクトル
    hidden = F.dropout(F.relu(self.fc1(feat_cat)), p=0.5, training=self.training)
    output = self.fc2(hidden)  # (n + 3)次元の出力ベクトル
    # 出力をβとTに分割
    beta_pred = output[:, :n]              # shapeパラメータβ（次元n）
    T_pred = output[:, n:]                # 並進ベクトルT（次元3: Tx, Ty, Tz）
    T_pred[:, 2] = F.softplus(T_pred[:, 2])  # Tz（奥行）は非負となるようSoftplus適用
    return beta_pred, T_pred

ここでfc1とfc2はそれぞれnn.Linear(1024 + attr_dim, 256)およびnn.Linear(256, n+3)で定義される全結合層です（attr_dimは属性ベクトル次元で、身長・体重・性別を使う場合3程度）。正面・背面から抽出した特徴ベクトル(各512次元)を結合し、さらにユーザ属性ベクトル（例: 3次元）も付加してから全結合層に通す構造となっています。例えば身長と体重を1次元ずつ、性別を1次元（男性0/女性1）で表し、それら3次元を属性ベクトルとして1024次元特徴に連結します。ネットワークはこれら追加情報も考慮してβおよびTを推定します。既知の身長・体重といった身体情報を与えることで形状推定精度を上げる先行研究に基づく工夫です。ポーズ情報についても、上述のように画像チャネルとしてエンコードしてCNNに入力しているため、空間的特徴抽出の段階で法線・深度・ポーズ・部位といった手がかりが融合されます。これにより、例えば腕の上げ下げによるシルエット変化をポーズヒートマップからネットワークが把握でき、純粋な体型（腕の太さや胴回りなど）の推定に専念できる効果があります。背面についても同様で、前後それぞれの姿勢情報を用いることで、片方の画像だけでは見えない関節や形状のヒントを補完できます（例: 背面画像からは肩甲骨周辺の形状が伺えるなど）。以上のように、本モデルでは正面・背面各画像の特徴＋各画像由来のポーズ・セグメンテーション特徴＋ユーザ属性を統合して、被写体の3D形状パラメータを推定します。
出力パラメータの扱い: 推論時はバッチサイズ1なので、beta_pred（形状パラメータベクトル, 次元$n$）とT_pred（平行移動ベクトル, 次元3）がそれぞれ得られます。これらを必要に応じてCPU上のNumpy配列に変換し、リストやJSON形式でAPIレスポンスに含めます。また、後続処理のためPyTorch TensorのままSTARモデルに入力し3Dメッシュを得る場合もあります（次項）。$T_{\text{pred}} = (T_x, T_y, T_z)$はカメラ座標系での人物重心位置（メートル単位）で、深度マップを身長基準で正規化しているため、$T_z$（奥行方向距離）は概ね実際のカメラ〜被写体距離に近い値となる想定ですが、正確な値合わせは後段で調整します（後述）。
STARモデルへの適用: PyTorch上にSTARモデルのカスタムレイヤー（STARLayer等）をロードし、推論時にそれを使って3D頂点座標群を計算します。例えば:

star_layer = STAR(gender='neutral', num_betas=n).to(device)
verts = star_layer(beta_pred, theta_default)  # 推定βと既定ポーズthetaからメッシュ頂点を計算

として、verts（サイズ: 1×6890×3のTensor, ワールド座標系）を取得します。ここでtheta_defaultは既定のAポーズ（24関節×3軸=72次元で各関節角度0の直立姿勢。両腕をやや開いた姿勢）です。まずは推定βとAポーズからメッシュを再構成し、続いてこれを推定$T$でカメラ座標系に配置します。正面ビューのメッシュは単純に

verts_front = verts + T_pred.view(1,1,3)

と各頂点に平行移動を加えます。背面ビューのメッシュは

R180 = rotation_matrix(axis='y', angle=180°)  # Y軸周り180度回転
verts_back = torch.bmm(verts, R180^T) + T_back

と計算します。ここで**$R_{180}$はY軸周り180°回転を表す3×3行列（正面→背面カメラへの剛体回転）、$T_{\text{back}}$は初期値として $-R_{180} \cdot T_{\text{pred}}$ を使用しTensor化したものです。この操作により、正面・背面それぞれのカメラ座標系における3Dメッシュ頂点集合（verts_front, verts_back）を得ます。なお本改訂版では、推定ポーズ情報を利用してメッシュ姿勢を入力画像に近づけることも検討しています。例えばSapiensの2Dキーポイントから被写体の大まかな3D姿勢を逆推定し（前後2視点の情報から三角測量する等）、STARモデルの関節角度thetaに反映することが理想です。しかし完全な3D姿勢推定には別途最適化が必要なため、本実装では推論直後点検用にSTARモデルのジョイント位置を投影して2Dキーポイントと比較する**程度に留めています。後述の最適化工程でポーズ微調整を行うことで、最終的には画像上の関節位置とメッシュ関節が合致するようにしています。
結果の利用: ここまで計算したメッシュは主に検証や可視化に使用します。例えばOpen3Dなどでverts_frontおよびSTARのfaces（三角形面リスト）を組み合わせて点群やメッシュを表示し、元画像と合っているか確認できます。また実装時には推論後にオーバーレイ画像を生成しています。具体的には、正面RGB画像に対しverts_frontを透視投影し、各関節点（STARモデルのジョイント位置）を2D上に描画して被写体に重ね合わせる処理を行いました。ポーズ情報を追加したことで、初期推定段階でもネットワークは姿勢による見かけの変化をある程度補正した形状を出力できており、腕や脚の位置も入力キーポイントに概ね整合していることを確認しています。局所的な微細ズレ（例: 腕が胴体に僅かに食い込む等）は後述の最適化工程で修正していきます。

	3.	メッシュとカメラ位置合わせ
深度による$T_z$補正: ネットワーク出力後、PyTorch3Dを使ってverts_frontをカメラビューでレンダリングし、モデルから見た深度マップ $D_{\text{model}}^{(front)}$ を得ます。これと入力深度マップ $D^{(front)}$（前処理済みでTorch Tensor化済み）を比較し、人物領域における平均深度差を求めます。具体的には人物マスクTensorを用いて

diff = (D_model_front[mask] - D_front[mask]).mean()

を計算します。diffが正ならモデルが実際より遠く、負なら近すぎることを意味するので、

T_pred[2] -= diff

と$T_z$（奥行き）成分を補正します。補正後は

verts_front = verts_front - diff

とZ軸方向に平行移動し、verts_backも同量だけZ方向に移動させます。これによりモデルの平均深度が実データと一致し、人物全体のスケール感がさらに整合します。
実身長によるスケール確認: 次に、補正後のverts_frontから被写体の3D身長を計算します。具体的には、

y_min = verts_front[:, :, 1].min()
y_max = verts_front[:, :, 1].max()
height_pred = y_max - y_min

として身長（モデル上のY軸方向の寸法）を算出します。これをユーザ入力の$H_{\text{real}}$と比較し、差が1%以上であれば一括スケール補正を行います。例えば

scale = H_real / height_pred
verts_front *= scale
verts_back *= scale
T_pred *= scale
T_back *= scale

のように、頂点座標および平行移動ベクトル全体を等倍スケーリングします。これにより出力メッシュの身長が正確に$H_{\text{real}}$になります。ただし、前段の処理でほぼ一致していることが期待されるため、この操作は微調整に留まります（もしネットワーク推定が大きく外れるケースでは警告を出す仕組みも検討できます）。実装上はチェックのみ行い、大きな差が無ければスケール補正はスキップしています。
背面位置合わせ: 背面メッシュverts_backについても、前面と同じ深度補正とスケール補正を適用しています。前面・背面で独立に$T_z$を推定していないぶん、調整は一括のみですが、初期仮定の180°回転が大きく外れていなければ問題なく整合します。必要に応じて背面画像に対してもPyTorch3Dレンダリングを行い、例えば人物シルエット同士を比較して$T_x, T_y$の微小な補正を行っても構いませんが、基本的にはネットワーク出力$T_{\text{pred}}$＋仮定の$T_{\text{back}}$で十分な結果が得られています（後段の最適化工程でさらに微調整します）。

	4.	最適化工程（オプション）
本工程は高精度を期す場合のオプションであり、refine=Trueの場合に実行します（デフォルトFalse）。推論結果をもとに、βやT（および必要に応じてポーズ角度θ）を微調整して画像との誤差を最小化します。
最適化対象と初期値: 推定βを微調整可能にするため、

beta_opt = beta_pred.detach().clone().requires_grad_(True)
T_opt = T_pred.detach().clone().requires_grad_(True)

としてそれぞれコピーし、勾配計算を有効にします。ポーズ角度（関節角θ）に関しては、今回Sapiensから2Dキーポイントを取得しているため、基本的には既知の姿勢情報があります。しかし、Sapiensの2D推定には若干の誤差が含まれる可能性や、深度方向の角度までは含まれていないことから、微調整の余地があります。そこでθの一部自由度も微調整対象に含め、姿勢ズレの補正を行います。具体的には、上半身前後屈・下半身前後屈・肩関節の開き具合・股関節の開き具合など、画像上の関節位置に影響しやすいパラメータを選択し、合計で6〜10次元程度の角度パラメータtheta_optを定義します。初期値はSTARの既定ポーズ角度（またはSapiens推定に基づき推測された角度）とし、requires_grad_(True)を設定します。例えば、Sapiensの2Dキーポイントで腕がほぼ下ろされていることがわかれば肩の初期角度をやや小さめに設定する、といった工夫をします（初期値設定が難しい場合は0開始でもLBFGS法で最適化可能です）。以上のパラメータリスト（[beta_opt, T_opt, theta_opt]）に対し、optimizer = torch.optim.LBFGS(..., lr=1.0, max_iter=20)を設定します。LBFGSは準ニュートン法で、反復回数が少なく済む傾向があり本タスクでは収束が速かったため採用しています（Adamでも問題ありません）。
損失関数計算: optimizer.step(closure)で呼ばれるclosure()関数内では、現在のbeta_opt, T_opt, theta_optからSTARレイヤーでメッシュ頂点verts_optを計算し、PyTorch3Dで前後両カメラからの法線マップ・深度マップ・シルエットをレンダリングします（$N_{\text{model}}^{(front/back)}, D_{\text{model}}^{(front/back)}, S_{\text{model}}^{(front/back)}$）。また、既に前処理済みの入力法線・深度・シルエット ($N^{(front/back)}, D^{(front/back)}, S^{(front/back)}$)を用意しておき、それらと比較して損失を求めます。各項は次の通りです:
	•	法線損失 $L_n$: 各画素について予測法線と入力法線の内積$\cos\theta$を計算し、人物領域で$(1 - \cos\theta)$の平均を取ります（内積が1で向き一致、0で直交、-1で逆向き）。実装上は

cos = (N_model * N_input).sum(dim=2) 
L_n = ((1 - cos) * mask.float()).mean()

のように計算します。

	•	深度損失 $L_d$: 各画素について予測深度と入力深度の絶対差を人物領域で平均します。実装上は

L_d = (((D_model - D_input).abs()) * mask.float()).mean()

のように計算します。

	•	シルエット損失 $L_s$（必要に応じて）: PyTorch3DのSilhouetteShaderで得た透過率マップ（擬似的なシルエット画像）$S_{\text{model}}$と人物マスク画像$S_{\text{input}}$（0/1の2値）との画素差の二乗和を人物領域全体で平均します。シルエットの不一致（はみ出しや不足）があると損失が増えるため、形状やポーズの調整が促されます。
	•	関節ポイント損失 $L_j$（本改訂版で追加）: STARモデル上の関節位置を投影して得られる2D位置と、Sapiens推論の2D関節キーポイント（入力ポーズ情報）との距離を測ります。具体的には各関節$i$について投影位置$(x^{model}_i, y^{model}_i)$と観測位置（Sapiens出力）$(x^{obs}i, y^{obs}i)$のユークリッド距離の二乗を計算し、その平均を取ります。人物マスク外で観測された関節（例: 片側の画像では見えない関節）は無視します。式で書けば、
$$L{j} = \frac{1}{|\mathcal{J}|} \sum{i \in \mathcal{J}} \big( (x^{model}_i - x^{obs}_i)^2 + (y^{model}_i - y^{obs}_i)^2 \big),$$
ただし$\mathcal{J}$は評価対象とする関節集合（両画像で見える主要関節）です。実装では各ビューそれぞれについてマスク内関節の誤差を計算し平均する形にしています。関節損失は、姿勢のズレや骨格長の違いによる投影位置の不一致を直接ペナルティするため、βやθの微調整に強く影響します。
以上の各損失を適切に正規化（平均）した後、重み付けして合計し全体の損失$L_{\text{total}}$とします。例えば初期値として$w_n = 1.0, w_d = 100.0, w_s = 2.0, w_j = 0.1$といった具合に重みを設定しました。深度誤差は数値のスケールがメートル単位と大きいため100倍程度で他と釣り合わせ、関節損失はピクセル単位の値（数十px誤差で1000程度）となるため小さめの係数を与えました。
loss.backward()で勾配を計算し（LBFGSではclosure()内でreturnしたlossに基づき内部でbackward処理が行われます）、最適化ステップを繰り返します。
更新と終了: LBFGSは内部でパラメータ更新と収束判定を行い、最大20回の反復で終了します。最適化後、beta_optやT_optの値を取り出して最終推定値とします。例えば beta_refined = beta_opt.detach().cpu().numpy() として保存し、後述の出力に利用します。T_optも同様です。またtheta_optも最終値を取得し、必要であればこれを最終メッシュ描画に使用します。最終的にこの結果に基づきもう一度STARでメッシュ頂点を計算・レンダリングし、最終法線・深度・シルエット・関節位置が入力とほぼ一致していることを検証します。実身長$H_{\text{real}}$に対しても、最終メッシュの身長を再計算して誤差が0.5%未満であることを確認しました（最適化中に深度および関節位置のズレを埋めるようβが更新され、体格全体が微調整されるため、自動的にスケールも合ってきます）。例えばあるテストでは、身長1.72mの人物に対し最適化前のメッシュ身長が1.70m（誤差約2cm）だったものが、最適化後に1.719m（誤差1mm未満）となりました。また、腕の開き具合など姿勢も入力写真とほぼ一致し、関節の投影誤差は平均2px程度まで低減されました。このように、最適化工程を通じて身体寸法推定と姿勢整合がさらに正確になります。

	5.	出力とレポート生成
βベクトル出力: 最終的なβベクトル（例えば次元$n=10$）をユーザ向けに提供します。内部ではこれをJSONシリアライズしてAPIレスポンスとする想定です。ユーザがSTARモデル環境を持つ場合にはこのβから同じ体型の3Dモデルを再現できます。体型パラメータという専門的な出力だけでなく、ユーザが理解しやすい情報も併せて提供します。
身体寸法の算出: 最終メッシュから身長・ウエスト周囲長・ヒップ周囲長等を計測し、テキスト出力します。身長は上述の$y_{\max}-y_{\min}$（m単位）を算出してcm換算し、小数1位まで表示します。ウエスト・ヒップ周囲長はメッシュ上で水平スライス断面を用いて計測します。具体的には、胴体部分の頂点群（例: 脇下～股下の範囲）を抽出し、複数の高さレベルで水平面との交線を求めます。各断面でその平面内の頂点をXZ平面に射影して凸包を計算し、その周長を算出します。全断面中の最小周長をウエスト、最大周長をヒップと見做します。これらをcm単位で出力します（例:「ウエスト推定: 78.5 cm」）。実装上は10〜20層程度スライスし、スプライン補間で連続的な断面輪郭を近似することで精度を高めました。なお、この計測結果はユーザへのフィードバックだけでなくシステム精度評価にも使用しています。例えば同一被写体を別日撮影した場合に推定ウエスト値が±1cm程度で再現されるか検証するなど、リピート性（再現性）の評価を行いました。
レポート表示: UIやデバッグ用に、入力画像に対する推定結果を視覚化する機能も実装しています。正面および背面のRGB画像に対し、推定メッシュをOpenGLレンダリングで半透明表示し元画像に重ねたり、メッシュのエッジ（アウトライン）を描画して人物シルエットと比較する、といった重畳表示を行います。また、2D関節キーポイント（入力の赤外線マーカーなど）と、最終メッシュから投影した関節位置を両方描画し、重なり具合を見る機能も追加しました。さらに、ウエストラインやヒップラインに相当するメッシュ上の箇所をハイライト表示し、その周長値を注記するなど、ユーザが直感的にフィット具合と数値を確認できる工夫も行っています。現段階ではMatplotlib等で2Dプロットする簡易な実装ですが、将来的にはWebブラウザ上で3Dモデルを回転・拡大できるビューアの組み込みも検討しています。

学習パイプライン詳細

本システムはモデル精度向上のため、現実のペアデータが少なくても効果的に学習できる合成データ生成と教師あり学習のパイプラインを備えています。手法的にはSTRAPS（BMVC 2020）のアイデアに倣い、シルエットや2Dジョイントの代わりに法線マップ・深度マップ（および本改訂版では関節ヒートマップ・セグメンテーションマップ）をプロキシ（代理観測）として用いて学習する点が特徴です。すなわち、大量の合成3D人体データからレンダリングした法線・深度・関節・セグといった擬似的な観測量を入力とし、対応する真の体型パラメータ（β）を出力ラベルとしてネットワークを訓練します。これにより現実データに頼らず高精度な推定器を獲得できます。以下、学習パイプラインの各ステップを詳述します。

① 合成データ生成
学習中にオンザフライ（逐次）でランダムな人体パラメータをサンプリングし、仮想的な正面・背面画像ペアを生成します。各サンプル生成手順は以下の通りです。
	•	形状パラメータβのサンプリング: STARモデルの形状空間からランダムなβベクトルを生成します。デフォルトではジェンダーニュートラル版STARの主成分空間に沿って各成分を独立に**標準正規分布 $\mathcal{N}(0,1)$**から抽出します。これにより平均体型を中心に広範な体型バリエーション（高身長～低身長、痩せ型～肥満型など）が網羅されます。必要に応じ、身長や体重の現実的な分布を反映するようサンプリングに重み付けすることも可能です（例えば第1主成分は±2σ以内に収める等）。また、50%の確率で男性モデル・女性モデルを用いるなど性別のランダム化も検討できますが、本仕様ではまずジェンダーニュートラルモデル単独で十分な性能を得る前提とし、必要なら性別ラベルを追加で扱えるよう拡張します。性別混在で学習する場合は、ミニバッチ内で男女モデルを切り替え、ネットワークに1bitの性別指示を与える方法が考えられますが、ここでは割愛します。なお、後述のように本システムでは身長を入力深度マップのスケール正規化で取り扱うため、**学習データ上の全被写体の身長は統一スケール（約1.7m相当）**となるよう調整しています。その結果、βサンプリング時に極端な高身長・低身長を選んでも、レンダリングされる画像上ではある程度規格化された大きさになります。ただしβ自体には相対的な身長差に対応する成分も含まれているため、ネットワークは形状特徴としての体型比率（例: 脚の長さの比率や肩幅など）を学習できます。ユーザの実身長が常に得られる運用であるため、本実装では学習データの深度を統一スケールに正規化する設計としましたが、将来的には身長値をネットワークへの追加入力とし、合成データ上でもその値をランダム付与することでネットワークが属性情報を条件として扱えるように拡張可能です（現時点でネットワークは属性入力および姿勢入力に対応済みですので、データ生成時に高さ値やポーズを変動させるだけで対応できます）。
	•	ポーズパラメータθのサンプリング: 基本となるAポーズ（直立で両腕をやや開いた姿勢）から微小～中程度の乱れを加えたポーズを生成します。各関節角度にガウスノイズを加えるイメージです。例として、両肩の横方向回旋に±10°、両肘に±5°、股関節に±10°、膝に±5°、首と腰の前後屈に±5°程度の範囲でランダムに変化させます。従来よりもやや大きめのポーズ変化を許容しているのは、本改訂版でネットワークがポーズ情報を入力として受け取るため、形状推定への悪影響なく姿勢多様性に対処できるからです。 すなわち、腕の上げ下げや軽い体のひねりといった姿勢差を学習データに含めても、ネットワークはその違いを関節ヒートマップやセグメンテーションを通じて認識できるため、誤った形状解釈をしにくくなっています。ただし依然、極端なポーズ変形（腕を真上に上げる、深くしゃがむ等）は除外し、直立に近い範囲に限定しています。また、人物全体のYaw回転（左右方向の向き）は正面・背面カメラとの相対関係を維持するため基本0°/180°に固定しますが、データ拡張として±10〜15°程度の全体Yaw回転を許容します。例えば正面向きに対し10°右に振ったら、背面ではちょうど反対向きの190°（または-170°）とする、といった具合です。この微調整により、前後カメラが完全な背中合わせ配置でない状況（撮影者の位置ズレなど）にもモデルが対応できるようにします。
	•	3Dメッシュ生成: サンプリングした$(\beta, \theta)$をSTARモデルに与え、対応する3Dメッシュを取得します。実装上はPyTorch用STARモデルローダを使用し、β（例:10次元）とθ（関節角度72次元程度）から頂点配列$\mathcal{V}$（サイズ6890×3）および面リスト$\mathcal{F}$を計算します。さらにメッシュの頂点法線$\mathcal{N}$も計算しておきます（面法線を各頂点で平均して算出）。この時点でメッシュはワールド座標系（人物重心が原点、人物正面がZ+方向）にあり、STARモデル内でポーズも適用済みです。身長などの絶対スケールはSTARモデル内で自動的に決まります（βに依存し、β=0で約1.7m）。得られた3D情報（頂点・法線）は以降の仮想レンダリングに用います。
	•	仮想カメラ設定: 正面ビュー・背面ビューに対応する2つの仮想カメラを設定します。正面カメラはワールド原点（人物中心）から見てZ+方向（人物正面側）に、背面カメラはZ-方向（人物背面側）に配置します。それぞれの具体的位置はランダムに変動させてデータ多様性を確保します。例えばカメラ距離$D$を2.5〜3.5mの一様分布からサンプリングします。正面カメラ位置は$(\Delta_x, \Delta_y, -D)$、背面カメラは$(-\Delta_x, \Delta_y, +D’)$とし（原点から見てZ軸正負方向に配置）、両者とも原点（人物中心）を向くように設定します（$\Delta_x, \Delta_y$は水平・鉛直方向のズレで、カメラ距離$D$に対して±2%程度の微小値をランダム付与します）。基本は$D’ \approx D$としますが5%程度の差は許容します。両カメラとも画角・解像度は実データに合わせます。例えば焦点距離50mm相当（垂直視野約30°）とし、画像解像度は512×512ピクセルに統一します。カメラ間の水平角度差は理想的には180°ですが、前述の通りデータ拡張として少しずらす場合があります。カメラのパラメータ（視野角、解像度、基準となる人物高さ）は学習時と推論時で揃えておく必要があります。本実装では、実際の撮影環境に合わせてカメラ設定を固定し（例えばスマホ背面カメラを想定して水平視野60°程度など）、その設定で合成データを生成しています。
	•	レンダリング: 仮想カメラから合成人物をレンダリングし、法線マップ・深度マップ・関節ヒートマップ・セグメンテーションマップを得ます。PyTorch3DなどのDifferentiable Rendererを用いてメッシュからピクセルごとの属性を計算します。
	•	法線マップ: まずメッシュ頂点法線$\mathcal{N}$を各カメラの座標系に変換します（頂点法線ベクトルにカメラの回転を適用）。それを頂点カラー値としてメッシュに割り当て、ラスタライズします。ラスタライズ時はPhongシェーディングではなく頂点カラーをそのまま補間表示するカスタムシェーダを使い、各ピクセルのRGBがカメラ座標系法線ベクトル（0〜1正規化エンコード後の値）になるようにします。具体的には、頂点法線$(n_x, n_y, n_z)$を$(\frac{n_x+1}{2},\ \frac{n_y+1}{2},\ \frac{n_z+1}{2})$に変換してRGBカラーとし、背景は黒(0,0,0)にします。出力はSapiensの法線マップと同形式（RGB各8bit, 法線成分0.5が無方向を表す）となるよう調整します。
	•	深度マップ: レンダリング時に得られるZバッファ値を利用して各ピクセルの深度（カメラ位置からの距離）を取得し、グレースケール画像にします。背景ピクセルにはデフォルト値0を割り当てます（Sapiensの深度出力も背景0の仕様）。深度値はシーン内の実スケール（メートル）ですが、そのままでは人物ごとに値域が異なるため正規化を行います。前処理と同様、人物の身長が常に一定になるようスケールします。具体的には、合成メッシュの身長（頂点Y座標のmax-min）を$H_{\text{sim}}$[m]とし、例えば$H_{\text{sim}}=1.8$のとき全深度値に$(1.7/1.8)$を乗じます。つまり全被写体が仮想身長1.7mになるよう線形スケーリングします。この基準値1.7mは訓練時の統一身長であり、推論時にも用いる値です。この操作により、合成深度マップも相対距離尺度に揃い、カメラ距離によらず深度レンジが統一されます。これは実画像での深度正規化（身長基準）と対応しており、学習時にネットワークがカメラ距離の影響を簡略化して学習できる利点があります（実運用ではユーザ身長が常に与えられるため、この方式で問題ありません）。一方でネットワークに身長を入力する拡張（既に実装済み）を用いる場合、学習時にはここで敢えて高さを揃えずランダムな$H_{\text{sim}}$で深度を生成し、その値を属性ラベルとしてネットワークに与える選択肢もあります。ただし本検証ではまず全データ統一スケールで学習させ、後述の微調整段階で高さ情報を導入して精度向上を図りました。
	•	関節ヒートマップ: 合成メッシュからSTARモデルのジョイントレグレッサ（関節位置計算機能）を用いて3D関節位置${J_i^{3D}}$を取得します（SMPLと同様STARにも6890頂点から24関節3D位置を算出する重み行列があります）。各関節位置を対応するカメラビューに射影し2D座標$(u_i, v_i)$を得ます。これを中心に据えた2Dガウシアンを描画し、関節ヒートマップ画像を生成します。前処理で述べた手法と同一です。各関節$i$に対し、空の画像に$\exp(-\frac{(x-u_i)^2 + (y-v_i)^2}{2\sigma^2})$を描画し、最大値が1となるよう強度を調整します。標準偏差$\sigma$は関節の検出精度に対応すると考え、ここでは5px程度とします（Sapiensの出力もそれに近い精度を想定）。K個の関節についてこれを行い、Kチャンネル分のヒートマップ画像を得ます。学習データ上ではSapiensではなく理想的な（誤差のない）関節位置が得られますが、後述のデータ拡張で検出誤差を模擬します。背景領域には描画しません。2視点（正面・背面）それぞれでこの処理を行い、正面用Kチャネル、背面用Kチャネルのヒートマップを生成します。
	•	セグメンテーションマップ: 合成メッシュの各頂点（および各フェイス）に部位ラベルを割り当て、ラスタライズ時にそのラベルで着色して表示します。部位ラベルはSMPLで一般的な14〜24部位の定義に従います（例: 背景0、頭1、胴2、左上腕3、右上腕4、左下腕5、右下腕6、左大腿7、右大腿8、左下腿9、右下腿10、左足11、右足12、左手13、右手14…といった分類）。STARの各頂点について、対応する最近傍の関節やボーンに基づき部位IDを割り振ります（公式に頂点→部位のマッピングがある場合はそれを使用します）。レンダリング時には、各フェイスの頂点カラーとしてその部位IDに応じた擬似RGBカラーを設定し、シャープなニアレスト補間でラスタライズします。出力されたカラー画像を各ピクセンについてIDに戻すことでラベルマップを得ます。背景ピクセルはID0（背景）となります。このラベルマップをCチャネルのワンホット形式に変換すれば、前処理と同様の部位セグメンテーション入力が得られます。
	•	背景マスク適用: レンダリングで得られた法線・深度・ヒートマップ・セグメンテーション画像は、現時点で背景部分がそれぞれ黒（法線/ヒートマップ）や0（深度/ラベル）になっています。必要に応じて、人物シルエット（背景=0/人物=1のマスク）を用いてこれらを改めてマスク処理し、背景を完全に無効化します（学習時はそこまで厳密にしなくとも、ネットワークが自動で無視するよう学習できますが、一貫性のため実施）。シルエットマスク自体はセグメンテーションラベルの背景対非背景で得られます。なお、人物シルエットの2値マスク画像も別途生成して保持します（損失計算等で使用）。
	•	データ拡張: 得られた法線・深度・ポーズ・セグメンテーションに対し、実データとのドメインギャップを埋めるため以下のランダム変換を適用します。
	•	法線ノイズ: 法線ベクトルについて各画素の方向をランダムに2〜5°程度揺らすノイズを加えます。実装的には法線画像のRGB値に微小なガウスノイズ（標準偏差σ=0.02程度）を加え再正規化する方法で近似します。これによりSapiens出力にも含まれ得る微細な予測誤差を模擬します。
	•	深度ブラー・ノイズ: 深度マップに3×3のガウシアンブラー（σ=1）で平滑化を施し、距離センサーの誤差を模擬します。また全画素に対し0.5〜1%程度のランダムな乗算ノイズ（誤差）を加えます。さらに16bit量子化程度の丸めノイズ（スケール$10^{-3}$程度の微小な段階値化）を付与することも検討しました。これらによりSapiens深度推定の不確実性を再現します。
	•	ヒートマップノイズ: 2D関節ヒートマップに対し、関節位置のわずかなブレを模擬します。具体的には各関節の中心位置を数ピクセル（例: 標準偏差2px）ランダムシフトさせてからガウシアンを描画します。また、ガウシアンのσ自体も関節毎にランダムに±1px程度変動させ、検出の確実性の差異を表現します。さらに、10%程度の確率で特定の関節ヒートマップをドロップアウト（ゼロ化）します。これは、たまに関節検出に失敗したり精度が極端に悪化するケースを模擬するためです（ただしSapiensは正面・背面両方あれば主要関節はほぼ検出できると期待されるため、完全ドロップは稀としています）。こうしたノイズ付加により、ネットワークは関節入力に多少の誤差や欠損があってもロバストに形状を推定できるようになります。
	•	セグメンテーションノイズ: 部位セグメンテーションマップに対し、境界部分を中心にランダムなゆらぎを加えます。例えば、確率5%で各画素のラベルを隣接する別ラベルに置換する（隣接部位が誤って張り出す）処理や、境界線上をランダムに膨張・収縮させる形で形状を微妙に変形させます。また、1%程度の画素を背景ラベルに置換する（人物領域内に欠損ホールが生じる）処理も加えました。これらにより、Sapiensのセグメンテーション出力で起こり得る微小な誤分類や欠損を再現します。
	•	背景ノイズ: マスク外の背景領域に、実画像風のランダム模様や色を合成します。人物には直接影響しませんが、境界ピクセル値に多様性を持たせることで境界付近の誤差に対するロバスト性を高めます。具体的には、ランダムなテクスチャ画像（Perlinノイズやボケた写真風画像）を背景に重ねたり、一様な照明グラデーションをかけたりしました。Sapiensは背景を黒出力しますが、実際のカメラ画像では背景に種々の物体や色が存在するため、そのギャップを埋める目的です（ネットワークには直接背景入力しないものの、前処理時に完全には除去しきれない値の影響を低減します）。
以上のデータ拡張はPyTorchのtransform（torchvision.transforms）やOpenCV処理で実装しています。なお画像全体のアフィン変換（回転・スケーリング）は適用しません。人物全体の位置や大きさを変えてしまうと、対応する真の3Dデータとの関係が崩れ、学習目標が不明瞭になるためです。
	•	入出力テンソル化: 最終的に、1つの合成サンプルから入力テンソル$\mathbf{X}$と教師ベクトル$\mathbf{y}$を構成します。入力$\mathbf{X}$は正面法線(3ch)+正面深度(1ch)+正面関節ヒートマップ(Kch)+正面セグメント(Cch) + 背面法線(3ch)+背面深度(1ch)+背面関節ヒートマップ(Kch)+背面セグメント(Cch)をチャネル方向にスタックしたTensorです（サイズ: $(2*(3+1+K+C)) \times H \times W$）。例えばK=17, C=14なら1サンプルの入力は${2*(3+1+17+14)}=70$チャネルの画像テンソルとなります。ただし実装上はこれを正面・背面2つの画像に分け、ネットワーク内部で別々に処理しているため、データローダ上は(正面: 35ch, 背面: 35ch)のペアとして扱います。教師信号$\mathbf{y}$は対応する真のSTAR形状パラメータβ（次元$n$）および真のグローバル位置$T_{\text{gt}}$です。ここで$T_{\text{gt}}=(T_x, T_y, T_z)$は正面カメラ座標系での人物重心位置で、合成データでは既知です。例えば正面カメラを$(0, \Delta_y, -D)$に置いた場合、人物重心はカメラ座標で$(0, -\Delta_y, D)$となるため、それを教師ベクトル$T_{\text{gt}}$とします（背面側の位置$T’{\text{gt}}$は推定対象外なので使いません）。さらに、ユーザ属性をネットワークで扱う場合は、各合成データに擬似的な身長・体重・性別情報を付与し、それも入力ラベルとします。具体的には、βから計算される身長$H{\text{sim}}$を「擬似身長ラベル」として、STARメッシュの体積からBMI22相当の標準体重を算出して「擬似体重ラベル」として与える方法です。また性別は50%の確率で男性/女性モデルを使う設定にしている場合はそのフラグ（0/1）をラベルに含めます。本実装では学習データ生成段階では性別は固定していましたが、属性入力拡張に合わせて性別もランダム化して再学習することも可能です。

② ネットワーク学習
合成データで構築された$(\mathbf{X}, \mathbf{y})$ペアを用いて形状推定ネットワークを教師あり学習します。主要な損失関数と学習設定は次の通りです。
	•	体型パラメータ損失 $L_{\beta}$: 推定βと真のβとの平均二乗誤差（MSE）です。式で書けば $L_{\beta} = \frac{1}{n}|\beta^{pred} - \beta_{\text{gt}}|^2$ となります。β各次元は（サンプリングにより）概ね$\mathcal{N}(0,1)$に正規化されているため、単純なMSEで問題ありません。STAR形状空間の固有値（分散）の大きさに応じて次元毎に重み付けすることも可能ですが、ここでは均等に扱います。$L_{\beta}$は本タスクの中心的目標であり、損失重み$w_{\beta}$を大きめ（例:1.0）に設定します。
	•	位置推定損失 $L_{T}$: 推定平行移動$T^{pred}$と真の$T_{\text{gt}}$との差のMSEです。すなわち $L_{T} = (T_x^{pred} - T_{x}^{gt})^2 + (T_y^{pred} - T_{y}^{gt})^2 + (T_z^{pred} - T_{z}^{gt})^2$ となります。ただし3成分それぞれのスケールが異なるため、特に奥行き方向$T_z$の誤差を重視します。具体的には$T_z$誤差に係数5〜10を掛けて損失に加算しました。これにより人物とカメラ距離の推定精度を高め、結果的にメッシュの絶対スケール推定が安定します。一方で$T_x, T_y$（左右・上下オフセット）は多少の誤差がβ推定に大きな影響を与えないため、そのままとします。なお学習データでは深度マップを身長基準1.7mに正規化しているため、$T_z$の真値も「被写体身長1.7m換算でのカメラ距離」に相当します（カメラ配置距離$D$に対し $T_z = D \times (H_{\text{sim}}/1.7)$ で計算）。推論時にはユーザ身長で同様の補正を行っているため、学習と推論で$T_z$のスケールは一貫しています。
	•	幾何学的一貫性損失 $L_{\text{geo}}$（オプション）: ネットワーク出力のβから再構成した3Dメッシュが入力法線・深度マップとどれだけ一致しているかを測る損失です。これは最終最適化工程の損失を学習時に一部取り入れる発想で、ネットワークが単に数値上βを合わせるだけでなく形状自体を合わせ込む方向に誘導できます。実装としては、合成データ生成時に既に得ている教師法線・深度 $(N^{gt}, D^{gt})$に対し、ネットワーク出力β（および規定ポーズ$\theta$）をSTARに通して得たメッシュからレンダリングした $(N^{pred}, D^{pred})$ を比較します。そして前述した法線損失$L_n$・深度損失$L_d$に相当する値を計算します。さらに人物シルエットのIoU（あるいは$L_s$相当の二乗誤差）も加味できます。これらを合計したものを$L_{\text{geo}}$とし、損失関数に組み込みます。ただし計算コストが大きいためデフォルトでは$L_{\text{geo}}$は使用せず、モデルがある程度収束した後のファインチューニング段階で限定的に適用しました。$L_{\text{geo}}$を導入することで、β推定を主目標にしつつも法線や奥行きの整合も意識した学習となり、わずかな精度向上が見込めます。
	•	総合損失関数: 上記を統合し、 $L_{\text{total}} = w_{\beta}L_{\beta} + w_T L_T + w_{\text{geo}}(L_n + L_d + L_s)$ とします（$w_{\text{geo}}$は適用時のみ非ゼロに設定）。例えば初期学習では $w_{\beta}=1.0, w_T=0.1, w_{\text{geo}}=0.0$ とし、十分収束した後に $w_{\text{geo}}=0.01$ を有効化して微調整するイメージです。これによりβ推定を主目標に据えつつ、距離推定と幾何整合もわずかに考慮した学習となります。本改訂で導入したポーズ・セグメンテーション情報については、それ自体は入力特徴であり教師ではないため直接的な損失項目はありません。間接的には、例えばネットワークが正しくβを推定すれば結果として予測メッシュの関節位置も正しくなるため、前述の$L_{\text{geo}}$（もしくは最適化工程）で評価されることになります。もし現実データで弱教師学習を行う場合には、観測2D関節に対する投影損失$L_j$を学習時にも課す選択肢がありますが、本システムでは合成データでβラベルを直接与えているため不要です。
	•	オプティマイザと学習率: 勾配最適化にはAdamを使用し、初期学習率を1e-4に設定しました。適宜コサインアニーリングやステップスケジューラで減衰（例: 5万イテレーションで1e-4→5e-5、8万で1e-5に低減）を行います。Weight Decay（L2正則化）を1e-5程度設定し汎化を促進します。混合精度学習（AMP）も有効にしてメモリ削減と演算高速化を図りました。ミニバッチサイズはGPUメモリと相談で16〜32とし、1エポックを合成データ1万サンプル相当とみなして合計10〜15エポック（=10万〜15万サンプル）学習しました。学習中は合成データを逐次生成しながらランダム供給します（全データを事前生成せずオンザフライ）。PyTorchのDataLoaderを用い、num_workers=4程度で並行してデータ生成＋前処理を行い、GPU計算とのパイプライン並列化を実現しました。
	•	検証とモデル保存: 学習中、一定間隔で検証用データによる評価を行います。検証用には例えばSenguptaらのSSP-3Dデータセットの一部（実画像に擬似GTの3D形状ラベルが付属）を用いるなど考えられますが、本実装では簡易的に合成データの別バッチで評価しました。指標としてはウエスト周囲長の平均絶対誤差（cm）やβベクトルのMSE等を用います。エポックごとにモデルパラメータを保存し、検証誤差が最良だった回のモデルを最終モデルとして採用しました。モデルとともに学習ハイパーパラメータ（使用した$n$や損失重み、学習率スケジュール等）も記録し、将来の再学習時に再現・比較できるようにしています。

③ 学習高速化の工夫
合成データ生成とレンダリングには計算コストが伴うため、以下の工夫で効率化しました。
	•	バッチレンダリング: PyTorch3Dのバッチレンダリング機能を活用し、ミニバッチ内の複数メッシュをまとめてレンダリングします。例えば16体分の合成データを一度にrendererに通すことで、GPU上での並列計算が可能です。これにより1体ずつ逐次処理するより高速になりました。特に法線・深度・セグメンテーションは同一メッシュに対しレンダリングパスを分けていますが、複数メッシュについては一括で処理できます。なお関節ヒートマップはレンダラを使わず数式描画で対応したため、計算コストは低いです。
	•	データローダ非同期化: torch.utils.data.DataLoaderをnum_workers=4で使用し、データ生成を並列スレッド/プロセスで非同期に行います。メインプロセスがGPUで学習計算中に他のワーカが次のバッチのSTAR計算やレンダリングを行い、処理待ち時間を減らします。各ワーカはプロセス開始時にSTARモデルやレンダラをロードしておき、毎回初期化するオーバーヘッドを避けています。
	•	STAR計算の効率化: STARモデル計算で繰り返し使用される定数（例: 三角形面リストやブレンドウェイト行列など）はメモリに保持し、毎回ディスクからロードしないようにしています。また、ポーズ変化が小さい設定のため線形ブレンドスキンニングの再計算コストも抑えられています。こうした工夫で1サンプルあたりの生成時間を100ms以下に収めています（マルチスレッド生成時）。
	•	AMPの活用: 学習時にはtorch.cuda.amp.autocastを用いて半精度（FP16）演算を行っています。特にレンダリング処理やCNNのフォワード計算で効果が大きく、メモリ消費削減とスループット向上に寄与しました。勾配適用時にはGradScalerでスケーリングすることで勾配消失や発散を防ぎつつ安定性も確保しています。

これらにより、合成データを逐次生成しつつでもGPUを高い使用率で稼働させ、現実的な時間（数日程度）で学習を完了できました。BMVC 2020のSTRAPSでは大規模合成データにより高精度を達成していますが、本システムも同様の手法で現実データに匹敵する性能を目指しています。

④ モデル更新と将来拡張
学習済みモデルは推論パイプラインに組み込み、ユーザからの画像入力に対して即座に体型推定を行います。今後、新たなデータや要件に応じてモデルを再訓練・改良することも可能です。例えば、形状パラメータ次元$n$を増やしてより詳細な体型表現を行う場合、設定を変更して再度合成データで学習します。その際、既存モデルの一部重み（低次元部分）を初期値として利用することで学習を安定化させることもできます。また、出力形式の拡張としてウエスト・ヒップ等の物理寸法を直接回帰するマルチタスク学習に発展させることも検討できます。実際にSmithらの研究（BfSNet）では2視点シルエットからの形状推定において既知の身長や体重を入力し、さらにメッシュ体積や関節位置を出力するマルチタスク学習で精度向上を図っています。本システムでもネットワークの全結合層出力を拡張し、βに加えて主要身体寸法（周囲長など）やジョイント3D位置を予測するノードを設ければ、合成データ上で真値寸法や真値ジョイントを教師に与えて学習できます。これによりファッション用途などではユーザに直接寸法値を提示することが可能となり、利便性が向上します。

さらに、入力データや属性情報の拡充も視野に入れています。側面画像の追加やスマホ搭載LiDAR深度の活用は既に前述しましたが、ユーザから取得できる身長・体重・性別・年齢といったメタ情報も推定精度向上に活かせます。例えばBfSNetでは人物の既知属性（性別や身長）をネットワークに入力することでシルエットからの体型推定精度を上げています。本システムでも同様に、高度な属性情報を推論時に入力パラメータとして受け取り、これらをネットワーク内部で活用することで推定の信頼性を高めています（本改訂時点ですでに身長・体重・性別の入力に対応済みです）。具体的には属性値を正規化した上でCNN特徴と結合し、ネットワークが身体特徴に応じた形状を出力できるようにしました。また学習時には合成データに擬似的な体重・BMI情報を割り当て、教師信号として利用しています（例: βから推定されるメッシュ体積と想定BMIとの差にペナルティを課す等）。ポーズ情報についても、本改訂でネットワーク入力として利用可能にしたことで、より幅広い姿勢に対する頑健性が得られました。将来的には単一視点入力時でも推定精度を維持するため、Sapiensの出力する2Dポーズやシルエットを用いて事前に対象の姿勢を把握し、クラウド側で形状推定モデルに適したビュー変換を行う等の工夫も考えられます。さらに、動画からのリアルタイム形状推定では、逐フレームの2Dポーズ追跡を統合して時系列的に安定した形状を推定するよう拡張することも視野に入ります。

現行設計では精度重視で推論パイプラインを構築していますが、今後はモデルの軽量化やリアルタイム化も視野に入れています。例えばSapiensモデルやResNetをモバイル向けに蒸留・圧縮したバージョンを使用し、スマートフォン上での動作を目指す計画もあります。その際は推論パイプラインを簡略化（1ビュー入力でも大まかに推定し、必要に応じクラウド連携で精細化する等）し、ユースケースに応じたモード（高速モード vs 精密測定モード）の提供も検討されます。また、入力に用いるセンサを拡張し、深度カメラやマルチビューからの情報を追加することで、より高精度な3D復元を目指すこともできます。本仕様に基づいて実装と調整を行えば、入力画像＋身長情報＋ポーズ情報から安定して高精度な3D人体モデル復元が実現できる見込みです。実装完了後はいくつかの検証用データで推定精度を測定し、要件を満たしているか確認します。例えば身長・ウエスト・ヒップが既知の被験者でテストし、誤差が許容範囲（±2〜3cm以内）に収まっていることを検証します。その上で、ユーザへの提供（例えばバーチャル試着サービスへの組み込み）に移行していきます。