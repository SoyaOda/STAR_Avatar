3D人体形状復元システム 実装仕様書（改訂版）

概要と使用技術

本システムは正面および背面のカラー画像から被写体の体型パラメータβ（STARモデル）を高精度に推定し、必要に応じてカメラ相対位置やポーズの微調整も行う3D人体形状復元システムです。従来手法であるDongらのDual-view法線マップベース復元法 ￼を踏襲しつつ、SMPLの代替としてSTARモデルを採用し、Meta社のSapiensモデルから得られる高精度な法線マップ・深度マップ・2Dポーズ（キーポイント）・部位セグメンテーションを入力情報として利用します。Sapiensは2Dポーズ推定・部位セグメンテーション・深度推定・法線予測を統合した大規模モデル（最大10億パラメータ級）で、300万以上の人物画像で自己教師あり学習されており、高解像度(1K)入力でも優れた精度と汎化性能を示します ￼。本システムの推論（Inference）パイプラインと学習（Training）パイプラインはともにPyTorchフレームワーク上に実装されており、GPUによる高速演算や自動微分、PyTorch3Dとの統合によってパフォーマンスと拡張性を両立する構成となっています。モデル規模はResNet18ベースで数百万パラメータ程度に抑えていますが、推論時はサーバーGPU上で実行することで高精度を確保しています（現状はリアルタイム性より精度を優先）。学習データ不足は合成データ生成で補い、STRAPS ￼にならったプロキシ表現（法線・深度マップおよびポーズ・セグメンテーション）による学習で効率化しています。主要な評価指標として、推定した3Dモデルから算出したウエスト・ヒップ周囲長の誤差を用い、被験者の身体寸法に対する精度を評価します。また、ユーザから提供可能な補助情報（身長や体重、性別など）は前処理の深度スケール補正に活用するとともに、本システムではこれら属性情報をネットワーク入力に組み込んで推定精度向上に役立てています（詳細は後述）。さらに、本改訂版では2Dポーズ情報（関節位置）や人物部位セグメンテーションも入力に加えることで形状推定の精度と安定性を向上させています（詳しくは推論パイプラインにて説明）。

以下では、使用ライブラリ・モデル、推論パイプライン、学習パイプライン、および実装上の詳細設定について説明します。

使用ライブラリ・モデル
	•	PyTorch (>= 2.x): 深層学習フレームワーク。CNNの構築・学習や自動微分、GPU活用を担います。本システムのモデルおよび合成データ生成・レンダリング処理はPyTorch上で実装しています。Mixed Precision Training (AMP) に対応しており、高解像度データの学習でもメモリ効率よく実行可能です。
	•	PyTorch3D: PyTorch用3Dレンダリング／3D操作ライブラリ。合成データ生成時にSTARメッシュから法線マップ・深度マップ・部位セグメンテーションマップをレンダリングするのに使用します。また微分可能レンダリングにより、学習時・推論時の最適化工程でモデル出力と画像（法線・深度・シルエット・2Dジョイント）との整合性評価が可能です。
	•	STARモデル実装: MPI提供のSTARモデルのPyTorch実装（公式GitHubのahmedosman/STAR等）を使用します。男女別およびジェンダーニュートラル版の.npzモデルファイルをロードし、推論・学習時に微分可能な形で利用します。STARは300次元の形状主成分空間を持ちますが、本実装では用途に合わせて必要な次元数のみを使用しています。PyTorch上でSTARの計算（シェイプブレンドシェイプ適用＋ポーズスキニング）をカスタムレイヤー化し、GPU上で効率的に動作させます。STARはSMPL互換の人体モデルですが、STAR特有のスパース補正項があるため、基本的には公式実装に沿って組み込んでいます（SMPL用コードからの流用も検討しましたが、差異があるため見送りました）。
	•	Sapiens (Meta AI): Meta社が公開している人間中心視覚モデルです ￼。人物画像から表面法線・奥行き（深度）・2Dポーズ（キーポイント）・部位セグメンテーションなどを高精度に推定できます。ViTベースのEncoder-Decoderアーキテクチャで学習されており、大量の人間画像で事前訓練されています ￼。本システムでは推論時にSapiensのTorchScript版（軽量化・推論専用のモデル）を使用し、入力RGB画像から高品質な法線マップ・深度マップ・人物マスク・2D関節位置（座標）を取得します。Sapiensは約10億パラメータの巨大モデルですが、推論専用に蒸留した“小型版”を利用することで、GPU上で1枚あたり約0.3秒の推論を実現しました（社内検証での実測値）。TorchScript化によりPythonがなくともC++環境で動作可能なため、将来的なモバイルデバイス搭載にも備えています。
	•	OpenCV / PIL: 画像の前処理やデータ拡張（リサイズ、ブラー、ノイズ付加、ヒートマップ生成など）に使用します。PyTorchのデータローダ（torchvision.transforms）と組み合わせ、リアルタイムに画像変換を行います。
	•	Hardware: NVIDIA Tesla V100/A100クラスのGPUサーバー上での動作を想定しています。推論時にはSapiensモデルやCNN推定器をGPUで実行し、リアルタイムに近い応答速度を得ます。学習時もすべてGPU上で計算を行い、バッチ並列計算で効率化します。推奨スペックはGPUメモリ16GB以上、CPUはマルチスレッド処理対応のものです。カメラ内部パラメータ（焦点距離等）は事前に較正して既知とします（例えば50mm相当の視野角約30°と仮定）。これにより身長と画像上のピクセル寸法からおおよその被写体までの距離を見積もり、前処理の深度スケール補正や後述のカメラ距離補正で利用します。

以上が主要な技術スタックです。次に、推論パイプラインの実装詳細を順に説明します。

推論パイプライン詳細

本節では、前述の概要フローに沿って推論処理各ステップを具体的に説明します。
	1.	前処理ステップ: 法線マップ・深度マップ・ポーズ情報生成
画像入力とSapiens推論: ユーザ入力のRGB画像（正面・背面）をまずPILで読み込み、所定の解像度にリサイズします（デフォルト512px、一時的に1024pxにも対応可）。次にSapiensモデルに入力し、表面法線マップ、深度マップ、および人物マスク、2D関節位置（キーポイント）、部位セグメンテーションマップを推論します。SapiensのTorchScript APIを利用し、例えば model.forward(rgb_tensor) のような関数で複数出力を同時に取得します。出力はPyTorchのTensor形式で得られるため（例: depth_tensor:形状1×1×H×W、normal_tensor:形状1×3×H×W、pose_tensor:形状1×K×2、seg_tensor:形状1×H×W）、これらを適宜Numpy配列に変換して画像として保存・処理できるようにします。具体的には、depth_tensorから各画素の深度値（カメラからの距離に相当）を取り出し、normal_tensorから各画素の3次元法線ベクトルを得ます。またpose_tensorからは各関節点の2次元画像座標（u,v座標）リストを取得し、seg_tensorからは各画素の部位カテゴリIDを得ます。Sapiensは人物領域外にも値を出力する可能性があるため、追加で人物マスク（1×H×WのTensor）を取得し、人物領域外のピクセルでは法線・深度・セグメンテーション値をマスクします。以上により、正面画像用に「法線3ch＋深度1ch＋ポーズKch＋セグメンテーションMch」のデータ（front_multichannel）、背面用にback_multichannelが得られます（Kは関節数、Mはセグメンテーションのチャネル数。後述）。
法線データの前処理: OpenCVで法線画像を読み込み（H×W×3配列として取得）、データ型をfloat32に変換してから /255.0 で[0,1]に正規化します。さらに normal = normal * 2 - 1 を画素毎に適用し、値域を（ほぼ）[-1,1]にリスケーリングします。念のため各画素について $\sqrt{n_x^2 + n_y^2 + n_z^2}$ を計算して単位法線ベクトルに正規化します（Sapiensの出力はほぼ正規化済みですが、数値誤差を補正）。背景ピクセル（人物マスク外）については、対応するnormal値を(0,0,0)に設定します（ネットワーク側でこの無効ベクトルが学習時に無視されるよう工夫しています）。
深度データの前処理: OpenCVで深度マップ（H×Wの1ch画像）を読み込み、float32に変換します。人物マスク外の深度値は0に設定します。一方、人物領域内の深度値はカメラ距離に比例した相対値なので、ここでスケール補正を行います。具体的には、正面画像について人物マスク内の画素のy座標の最大値・最小値を取得して人物の画面内高さ $H_{px}$ を算出します。ユーザから入力された実身長 $H_{\text{user}}$（単位m）がある場合、それを被写体の真の高さ $H_{\text{real}}$ とみなし、スケーリング係数 $S = H_{\text{real}} / H_{px}$ を計算し、深度マップ全体に $depth = depth * S$ を乗じます。これにより、画像内での人物の高さが $H_{\text{real}}$ に対応するスケールで深度が調整されます（例えば、画素高さ $H_{px}=900$px・実身長 $H_{\text{real}}=1.8$mなら $S \approx 0.002$ となり、深度値を0.2%倍して人物の頭頂～足底までの深度差が約1.8に揃うイメージです）。背面画像についても同様に処理します（同一人物なので $H_{\text{real}}$ は共通ですが、画像内高さ $H_{px}$ は微妙に異なる場合があるため別途計算して適用します）。この深度正規化により、ネットワークに与える深度マップは全被写体でスケールが統一されたものとなり、異なるカメラ距離の影響は主に後段で推定する $T_z$（奥行き方向並進）に反映できるようになります。実装上、ユーザが $H_{\text{user}}$ を提供しないケースは想定していませんが、もし未入力ならデフォルト値 $H_{\text{real}}=1.7$m として同様の計算を行います。
2Dポーズデータの前処理: Sapiensから出力された各関節点の画像座標リスト（例えばK=17点の (u_i, v_i)）を元に、関節ヒートマップを生成します。各関節について、出力画像と同じ解像度の2次元配列を用意し、対応する座標位置にガウス分布のピークを配置します（例: σ=3～5ピクセル程度の2Dガウシアンを描画）。具体的には、各ヒートマップチャネルは全画素が0の状態から開始し、関節位置を中心に値1のガウスカーネルを加えます。ガウス分布外では0となるため、ヒートマップは関節近傍のみ正の値を持つ形となります。K個の関節に対してそれぞれ1チャンネルのヒートマップ画像を作成し、これらをスタックすることで Kチャネルのポーズヒートマップ データを構築します。なお、Sapiensの推論結果に信頼度スコアが含まれる場合は、低信頼度の関節についてはヒートマップ強度を下げる、あるいは該当チャネルをゼロマップにする、といった処理も考えられますが、本実装では基本的に全関節を均等に扱います。人物マスク外の領域ではヒートマップ値は0となっているため、背景には影響しません。
セグメンテーションデータの前処理: Sapiensから得られた部位セグメンテーションマップ（H×Wの各画素が部位IDを持つ画像）も入力特徴として利用可能です。扱い方としては、ピクセル値がカテゴリIDであるラベル画像をOne-hotエンコードし、各部位ごとに1チャンネルのバイナリマスク画像を作成します（例えば背景、頭部、胴体、両腕、両脚など計MクラスならMチャンネルのマスク）。ただし、法線や深度マップですでに人物輪郭情報は得られているため、追加のセグメンテーションチャンネルは必須ではありません。本システムでは基本的に人物シルエット（背景マスク）を1チャンネルの入力として用い、必要に応じて詳細な部位マスクも検討する方針です。人物マスクはSapiensの出力を直接利用できるため、背景領域のピクセルは0（黒）、人物領域は1（白）の画像として取得します。これをそのまま1チャンネルのTensorデータとして追加します（前述の法線・深度マップにも適用済みで背景が0になっていますが、明示的にマスクチャンネルを与えることでネットワークが人物領域を認識しやすくなります）。
ネットワーク入力形式への変換: 上記処理により、正面用は (法線3 + 深度1 + ポーズK + シルエット1) チャンネルのTensor、背面用も同様のチャネル構成のTensorが得られます。実装ではこれらを torch.from_numpy でPyTorch Tensor（形状: C×H×W）に変換し、 torch.unsqueeze(0) でバッチ次元（サイズ1）を追加します。ここでC = 4 + K + 1（+ M’ オプション）です（M’は部位セグメントを細かく用いる場合の追加チャネル数）。例えばK=17で詳細セグメントは使わずシルエットのみの場合、C=4+17+1=22チャネルとなります。これらをPyTorchモデルに入力できる形式（batch_size=1, channels=C, H, W）として保持します。また、ユーザ提供の身長・体重・性別などの属性情報があればここで取得し、後述するネットワークへの付加入力として準備します（数値は正規化やエンコードを行った上でTensor化します）。例えば、身長Hは前述の深度スケール補正に利用していますが、ネットワークにも入力特徴量として渡すため、基準身長1.7m比での比率 ($H_{\text{user}}/1.7$) を計算してTensorに保持します。同様に体重Wは平均体重からの差分比やBMI値などに変換して保持し、性別はMale=0/Female=1のワンホットエンコードを行います。これら属性Tensorはモデル内部で画像特徴と結合して利用されます（詳細は次項）。
	2.	形状推定ネットワーク: モデル構造と属性・ポーズ統合
ネットワーク構造: 形状推定のニューラルネットワークはPyTorchにより torch.nn.Module として実装されており、正面・背面の2視点入力に対し重みを共有したResNetエンコーダで特徴抽出を行います。具体的には resnet = torchvision.models.resnet18(pretrained=True) でResNet18モデルをロードし、入力層を resnet.conv1 = nn.Conv2d(C, 64, kernel_size=7, stride=2, padding=3, bias=False) のように Cチャネル対応に置換します（ImageNet学習済みRGBフィルタの平均値を第4チャネル以降にも複製するなどして初期化するか、追加チャネル分はゼロ初期化します）。ここでCは前述の入力チャネル総数（例:22）です。BatchNorm層や以降のブロック構造は変更しません。ResNetの最終出力は512次元の特徴ベクトル（global average pooling後）なので、推論時のフローは以下のようになります：

def forward(self, front_input, back_input, attr_input=None):
    feat_f = self.resnet_extractor(front_input)   #  (Cch) -> 512dim 特徴ベクトル
    feat_b = self.resnet_extractor(back_input)    #  背面も同じResNetで処理（重み共有）
    feat_cat = torch.cat([feat_f, feat_b], dim=1) #  結合して1024dimに
    # ユーザ属性があれば特徴に連結
    if attr_input is not None:
        feat_cat = torch.cat([feat_cat, attr_input], dim=1)  # attrは適宜拡張・正規化済みベクトル
    hidden = F.dropout(F.relu(self.fc1(feat_cat)), p=0.5, training=self.training)
    output = self.fc2(hidden)  # (n + 3)次元の出力ベクトル
    # 出力分割
    beta_pred = output[:, :n]              # shapeパラメータβ（次元n）
    T_pred = output[:, n:]                # 並進ベクトルT（次元3: Tx, Ty, Tz）
    T_pred[:, 2] = F.softplus(T_pred[:, 2])  # Tz（奥行）は非負になるようSoftplus適用
    return beta_pred, T_pred

ここで resnet_extractor はResNet18から最後のAvgPool後の512次元ベクトルを得る部分です（ResNetのfcは使いません）。fc1 と fc2 はそれぞれ nn.Linear(1024 + attr_dim, 256) および nn.Linear(256, n+3) で定義しています（attr_dim は属性ベクトル次元で、例として身長・体重・性別を使う場合3程度となります）。上記により、正面・背面画像から抽出した特徴を結合し、さらにユーザ属性ベクトルも付加してから全結合層に通す構造となっています。さらに本改訂では、正面・背面画像の特徴にはそれぞれポーズヒートマップやシルエットマスク由来のチャネルも含まれているため、ネットワークは画像の外形・質感情報（法線・深度）と骨格位置情報（2D関節ヒートマップ）を同時に活用できます。これは、既知の関節位置などの情報を与えることで形状推定精度を上げる先行研究に基づいています。また、2視点分のポーズ情報を与えることで、片方の視点で見えない関節ももう片方の視点で補完され、形状推定に寄与します。キーとなる設計判断として、本実装では関節座標を直接MLPに入力するのではなく、ヒートマップ画像としてCNNの入力チャネルに含めました。これにより、各関節位置の空間的な相対関係を畳み込みで捉えやすくなり、画像のピクセル情報と統合的に特徴抽出できます。仮に関節座標のみをベクトルで与えた場合、画像特徴との関連付けが難しくなるためです。
出力パラメータの扱い: 推論時はバッチサイズ1なので、beta_pred（形状: 1×n）とT_pred（1×3）がそれぞれ得られます。これらをCPU上のNumpy配列に変換し、リストやJSON形式でAPIレスポンスなどに提供します。また後続処理のためPyTorch上でSTARモデルに入力することもあります。$T_{\text{pred}}$ については $(T_x, T_y, T_z)$ がカメラ座標系での並進量（メートル単位）です。深度マップを身長基準で正規化しているため、$T_z$（奥行方向距離）は概ね実際のカメラ〜被写体距離に近い値となる想定ですが、正確な値合わせは後段で調整します（後述）。
STARモデルへの適用: PyTorch上にSTARモデルのレイヤー（例えば STARLayer）をロードし、推論時にもそれを使って3D頂点座標群を計算します。例えば:

star_layer = STAR(gender='neutral', num_betas=n).to(device)
verts = star_layer(beta_pred, theta_default)  # 推定βと既定ポーズthetaからメッシュ頂点群を計算

として、verts（サイズ: 1×6890×3のTensor、ワールド座標系）を取得します。ここで theta_default は既定のAポーズ（24関節×3軸=72次元、各関節角度0に近く腕を軽く開いた姿勢）の角度列です。STAR計算により3Dメッシュの頂点群が得られたら、これを推定 $T$ でカメラ座標系に配置します。正面ビューのメッシュは単純に verts_front = verts + T_pred.view(1,1,3) と各頂点に加算します。背面ビューのメッシュは verts_back = torch.bmm(verts, R180^T) + T_back と計算します。ここで $R_{180}$ はY軸周り180°回転の3×3行列（正面→背面への回転を表現）、$T_{\text{back}}$ は初期値として $-R_{180} \cdot T_{\text{pred}}$ を使用しTensor化したものです。この操作により、正面・背面それぞれのカメラ座標系における3Dメッシュ頂点集合 (verts_front, verts_back) を得ます。
結果の利用: ここまで計算したメッシュは主に検証や可視化に使用します。例えばOpen3Dなどで verts_front およびSTARの faces（三角形面リスト）を組み合わせて点群やメッシュを表示し、元画像と合っているか確認できます。また実装時には、推論後にオーバーレイ画像を生成しています。具体的には、正面RGB画像に対し verts_front を透視投影し、各関節点（STARモデルのジョイント位置）を2D上に描画して被写体に重ね合わせる、といったことを行いました。ポーズ情報を入力に用いた効果もあり、初期推定段階でも体格や身長スケール感はユーザ入力の $H_{\text{real}}$ によって概ね一致し、関節位置の整合も向上しています。例えば腕や脚の長さ・位置が元画像の関節推定とほぼ対応していることを確認できます。局所的な微細ズレ（例: 腕と胴体の境界付近の食い込み）は後述の最適化工程で詰める想定です。

	3.	メッシュとカメラ位置合わせ
深度による $T_z$ 補正: ネットワーク出力後、PyTorch3Dを使って verts_front をカメラビューでレンダリングし、深度マップ $D_{\text{model}}^{(\text{front})}$ を得ます。これと入力深度マップ $D^{(\text{front})}$（前処理済み、Torch Tensor化済み）を比較し、人物領域における平均深度差を求めます。具体的には人物マスク（Torch Tensor）を用いて diff = (D_model_front[mask] - D_front[mask]).mean() を計算します。diff が正ならモデルが実際より遠くにあり、負なら近すぎることを意味するので、T_pred[2] -= diff で奥行き方向オフセットを補正します。補正後は verts_front = verts_front - diff とZ軸方向に平行移動し、verts_back も同じ量だけZ方向に移動させます。これにより、モデルの平均深度が実データと一致し、人物全体のスケール感がさらに整合します。
実身長によるスケール確認: 次に、補正後の verts_front から被写体の3D身長を計算します。具体的には、y_min, y_max = verts_front[:, :, 1].min(), verts_front[:, :, 1].max() を求め、height_pred = y_max - y_min とします。これをユーザ入力の $H_{\text{real}}$ と比較し、差が1%以上であれば一括スケール補正を行います。例えば scale = H_real / height_pred を計算し、verts_front *= scale、verts_back *= scale と頂点座標全体を等倍スケーリングします。同時に T_pred や T_back も同じ係数で乗じます。これにより出力メッシュの身長が正確に $H_{\text{real}}$ になります。もっとも、前段の処理でほぼ一致していることが期待されるため、この操作は微調整に留まります。実装上はチェックのみ行い、大きな差が無ければスケール補正はスキップしています（もしネットワーク推定が大きく外れるケースでは警告を出す仕組みも検討できます）。
背面位置合わせ: 背面メッシュ verts_back についても、前面と同じ深度補正とスケール補正を適用しています。前面・背面で独立に $T_z$ を推定していないぶん、調整は一括のみですが、初期仮定の180°回転が大きく外れていなければ問題なく整合します。必要に応じて背面画像に対してもPyTorch3Dレンダリングを行い、シルエットなどを比較してわずかな $T_x, T_y$ 補正を行っても構いませんが、基本的にはネットワーク出力 $T_{\text{pred}}$＋仮定の $T_{\text{back}}$ で十分な結果が得られています。
	4.	最適化工程（オプション）
本工程は高精度を期す場合にのみ有効化するオプションであり、refine=True の場合に実行します（デフォルトFalse）。
最適化対象と初期値: 推定 $\beta$ を微調整可能にするため、beta_opt = beta_pred.detach().clone().requires_grad_(True) としてコピーし、勾配計算を有効にします。同様に T_opt = T_pred.detach().clone().requires_grad_(True) を用意します。ポーズ（関節角度）に関しては基本Aポーズからのズレが小さいと想定されるため省略してもよいですが、実装上は上半身・下半身の前後屈（背骨と大腿のZ軸回り回転）程度は許容するため変数に含めます。例えば theta_opt として肩・股・首など主要部位の回転角をパラメータ化し（計6～10次元程度）、微調整対象に加えます。theta_opt の初期値はゼロ（既定ポーズ角度）です。以上をリストにまとめ、optimizer = torch.optim.LBFGS([beta_opt, T_opt, theta_opt], lr=1.0, max_iter=20) を設定します。LBFGSは準ニュートン法で反復回数が少なく済む傾向があり、本タスクでは収束が速かったため採用しています（Adamでも問題ありません）。
損失関数計算: optimizer.step(closure) で呼ばれる closure() 関数内では、現在の beta_opt, T_opt, theta_opt からSTARレイヤーでメッシュ頂点 verts_opt を計算し、PyTorch3Dで前後の法線マップ・深度マップ・シルエット （$N_{\text{model}}^{(front/back)}, D_{\text{model}}^{(front/back)}, S_{\text{model}}^{(front/back)}$）をレンダリングします。さらに、STARモデルのジョイントレグレッサを用いて verts_opt から3D関節位置を算出し、カメラ射影して2Dジョイント位置 $\mathbf{p}_{\text{model}}$ も得ます。これらと前処理済みの入力法線・深度・マスク・2Dジョイント（$N^{(front/back)}, D^{(front/back)}, S^{(front/back)}, \mathbf{p}$）を比較して損失を求めます。それぞれ:
	•	法線損失: 各画素について予測法線と入力法線の内積 $\cos\theta$ を計算し、人物領域で $(1 - \cos\theta)$ の平均を取ります。【計算例】cos = (N_model * N_input).sum(dim=2) ⇒ L_n = ((1 - cos) * mask.float()).mean()。
	•	深度損失: 各画素について予測深度と入力深度の絶対差を人物領域で平均します。【計算例】L_d = (((D_model - D_input).abs()) * mask.float()).mean()。
	•	シルエット損失: PyTorch3DのSilhouetteShaderで得た透過率マップ $S_{\text{model}}$ と人物マスク画像 $S_{\text{input}}$（0/1値）とのピクセル差の二乗和を平均します（いわゆるIoUに類する指標）。
	•	ジョイント位置損失: 入力の2D関節位置 $\mathbf{p}$（Sapiens出力）と予測メッシュから得た2D関節位置 $\mathbf{p}_{\text{model}}$ とのユークリッド距離を各関節で計算し、その二乗和を平均します。【計算例】L_j = || \mathbf{p}_{model} - \mathbf{p} ||_2^2 を全関節平均。また、奥行き方向についても3Dジョイント間距離で微調整する場合は3次元距離で計算しても構いません。
これらを合計し全体の損失 $L_{\text{total}} = w_n L_n + w_d L_d + w_s L_{\text{sil}} + w_p L_j$ とします。実装では例えば $w_n=1.0, w_d=100.0, w_s=2.0, w_p=10.0$ といった重み付けで法線方向の誤差・深度誤差・シルエット誤差・関節位置誤差を調整しています。最後に loss.backward() で勾配を計算します（LBFGSではclosure関数内でreturnしたlossに基づきoptimizerが内部でbackward処理を行います）。
更新と終了: LBFGSは内部でパラメータ更新と収束判定を行い、最大20回の反復で終了します。最適化後、beta_opt や T_opt の値を取り出して最終推定値とします。例えば beta_refined = beta_opt.detach().cpu().numpy() として保存し、後述の出力に利用します。T_opt も同様です。最終的にこの結果に基づきもう一度STAR→メッシュ→レンダリングを行い、最終法線・深度・シルエット・2Dジョイント位置が入力とほぼ一致していることを確認します。実身長 $H_{\text{real}}$ に対しても、最終メッシュの身長を再計算して誤差が0.5%未満であることを検証しました（最適化過程で深度・シルエット・関節位置を合わせ込むことで自動的にスケールも合ってきます）。例えばあるテストでは、身長1.72mの人物に対し最適化前のメッシュ身長が1.70m（誤差約2cm）だったものが、最適化後に1.719m（誤差1mm未満）となりました。これは最適化中に深度のズレ（約1%）を埋めるよう $\beta$ が更新され体格全体が微調整されたためです。このように最適化工程を通じて身体寸法推定がさらに正確になります。
	5.	出力とレポート生成
βベクトル出力: 最終的なβベクトル（例えば次元 $n=10$）をユーザに提供します。内部ではこれをJSONシリアライズしてAPIレスポンスとする想定です。また、ユーザがSTARモデル環境を持つ場合にはこのβから同じ体型の3Dモデルを生成できます。体型パラメータという専門的な出力だけでなく、ユーザが理解しやすい情報も併せて提供します。
身体寸法の算出: 最終メッシュから身長・ウエスト周囲長・ヒップ周囲長を計測し、テキスト出力します。身長は上述の $y_{\max}-y_{\min}$（m単位）を算出してcm換算し、小数1位まで表示します。ウエスト・ヒップ周囲長はメッシュ上で水平スライス断面を用いて計算します。具体的には、胴体部分の頂点群（例: 脇下～股下の範囲）を抽出し、複数の高さレベルで水平面との交線を求めます。各断面で、その平面内の頂点をXZ平面に射影して凸包を計算し、その周長を算出します。全断面中の最小周長をウエスト、最大周長をヒップと見做します。これらをcm単位で出力します（例:「ウエスト推定: 78.5 cm」）。実装上は10〜20層程度スライスし、スプライン補間で連続的な断面輪郭を近似することで精度を高めました。なお、この計測結果はユーザへのフィードバックだけでなく、システム精度評価にも使用しています。例えば同一被写体を別日撮影した場合に推定ウエスト値が±1cm程度で再現されるか検証するなど、リピート性の評価を行いました。
レポート表示: UIやデバッグ用に、入力画像に対する推定結果を視覚化する機能も実装しています。正面および背面のRGB画像に対し、推定メッシュをOpenGLレンダリングで半透明表示し元画像に重ねたり、メッシュのエッジ（アウトライン）を描画して人物シルエットと比較する、といった重畳表示を行います ￼。さらに、ウエストラインやヒップラインに相当するメッシュ上の箇所をハイライト表示し、その周長値を記載するなど、ユーザが直感的にフィット具合と数値を確認できる工夫も可能です。現段階ではMatplotlib等で2Dプロットする簡易な実装ですが、将来的にはWebブラウザ上で3Dモデルを回転・拡大できるビューアの組み込みも検討しています。

学習パイプライン詳細

本システムはモデル精度向上のため、現実のペアデータが少なくても効果的に学習できる合成データ生成と教師あり学習のパイプラインを備えています。手法的にはSTRAPS ￼（BMVC 2020）のアイデアに倣い、シルエットや2Dジョイントの代わりに法線マップ・深度マップ（および本改訂では2Dポーズ・セグメンテーション）をプロキシ（代理観測）として用いて学習する点が特徴です。すなわち、大量の合成3D人体データからレンダリングした法線・深度・ポーズ・部位マップ（擬似的な観測量）を入力とし、対応する真の体型（β）を出力ラベルとしてネットワークを訓練します。これにより現実データに頼らず高精度な推定器を獲得できます ￼。以下、学習パイプラインの各ステップを詳述します。

① 合成データ生成

学習中にオンザフライ（逐次）でランダムな人体パラメータをサンプリングし、仮想的な正面・背面画像ペアを生成します。各サンプル生成手順は以下の通りです。
	•	形状パラメータβのサンプリング: STARモデルの形状空間からランダムなβベクトルを生成します。デフォルトではジェンダーニュートラル版STARの主成分空間に沿って各成分を独立に**標準正規分布 $\mathcal{N}(0,1)$**から抽出します。これにより平均体型を中心に広範な体型バリエーション（高身長～低身長、痩せ型～肥満型など）が網羅されます。必要に応じ、身長や体重の現実的な分布を反映するようサンプリングに制限を加えることも可能です（例えば第1主成分は±2σ以内に収める等）。また、50%の確率で男性モデル・女性モデルを用いるなど性別のランダム化も検討しましたが、本仕様ではまずジェンダーニュートラルモデル単独で十分な性能を得る前提とし、必要なら性別ラベルを追加で扱えるよう拡張します。性別混在で学習する場合は、ミニバッチ内で男女モデルを切り替え、ネットワークに1bitの性別指示を与える方法が考えられますが、ここでは割愛します。なお、後述のように本システムでは身長を入力深度マップのスケール正規化で取り扱うため、**学習データ上の全被写体の身長は統一スケール（約1.7m相当）**となるよう調整しています。その結果、βサンプリング時に極端な高身長・低身長を選んでも、画像上ではある程度規格化された大きさで学習されます。ただしβ自体には身長差に対応する成分も含まれているため、ネットワークは形状特徴としての相対的な身長差（例: 脚の長さの比率や肩幅など）を学習できます。ユーザの実身長が常に得られる運用であるため、本実装では学習データの深度を統一スケールに正規化する設計としましたが、将来的には身長値をネットワークへの追加入力とし、合成データ上でもその値をランダム付与することでネットワークが属性情報を条件として扱えるように拡張可能です（本仕様改訂時点で既にネットワークは属性入力に対応済みですので、データ生成時に高さ値を変動させるだけで対応できます）。
	•	ポーズパラメータθのサンプリング: 基本となるAポーズ（直立で両腕をやや開いた姿勢）から微少な乱れを加えたポーズを生成します。各関節角度にガウスノイズを加えるイメージです。例として、両肩の横方向回転に±5°、両肘に±3°、股関節に±5°、膝に±2°、首と腰の前後屈に±3°程度の範囲でランダム変化させます。これにより実際の撮影時に起こり得る軽微な姿勢ズレを再現し、ネットワークがわずかなポーズ違いに頑健になるようにします。ただし極端なポーズ変形（腕を真上に上げる、しゃがむ等）は除外し、あくまで直立に近い範囲に限定します。また、人物全体の回転（Yaw方向）は正面・背面カメラとの相対関係を維持するため基本0°/180°に固定しますが、データ拡張として±5〜10°程度の全体Yaw回転を許容します。例えば正面向きに対し5°右に振ったら、背面ではちょうど反対向きの185°（または-175°）とする、といった具合です。この微調整により、前後カメラが完全な背中合わせ配置でない状況（撮影者の位置ズレなど）にもモデルが対応できるようになります ￼。なお、Sapiensから得られる2Dポーズ情報を有効活用するため、学習時のポーズバリエーションはあくまで微調整に留め、ネットワークが大きなポーズ変化に煩わされないようにしています。適度なばらつきの範囲であれば、関節ヒートマップを入力するネットワークは姿勢の違いを正しく解釈して形状推定に反映できます。
	•	3Dメッシュ生成: サンプリングした $(\beta, \theta)$ をSTARモデルに与え、対応する3Dメッシュを取得します。実装上はPyTorch用STARモデルローダを使用し、β（例:10次元）とθ（関節角度72次元程度）から頂点配列 $\mathcal{V}$（サイズ6890×3）および面リスト $\mathcal{F}$ を計算します。さらにメッシュの頂点法線 $\mathcal{N}$ も計算しておきます（面法線を各頂点で平均して算出）。この時点でメッシュはワールド座標系（人物重心が原点、人物正面がZ+方向）にあり、STARモデル内でポーズも適用済みです。身長などの絶対スケールはSTARモデル内で自動的に決まります（βに依存し、β=0で約1.7m）。得られた3D情報（頂点・法線）は以降の仮想レンダリングに用います。
	•	仮想カメラ設定: 正面ビュー・背面ビューに対応する2つの仮想カメラを設定します。正面カメラはワールド原点（人物中心）から見てZ+方向（人物正面側）に配置し、背面カメラはZ-方向（人物背面側）に配置します。それぞれの具体的位置はランダムに変動させてデータ多様性を確保します。例えばカメラ距離 $D$ を2.5〜3.5mの一様分布からサンプリングします。正面カメラ位置は $(\Delta_x, \Delta_y, -D)$、背面カメラは $(-\Delta_x, \Delta_y, +D’)$ とし（原点から見てZ軸正負方向に配置）、両者とも原点（人物中心）を向くように設定します（$\Delta_x, \Delta_y$ は水平方向・垂直方向のずれで、カメラ距離 $D$ に対して±2%程度（数cm〜十数cm）をランダム付与します）。基本は $D’ \approx D$ としますが5%程度の差は許容します。両カメラとも画角・解像度は実際のデータに合わせます。例えば焦点距離50mm相当（垂直視野約30°）とし、画像解像度は512×512ピクセルに統一します。カメラ間の水平角度差は理想的には180°ですが、前述の通りデータ拡張として少しずらす場合があります。カメラのパラメータ（視野角、解像度、基準となる人物高さ）は学習時と推論時で揃えておく必要があります。本実装では、実際の撮影環境に合わせてカメラ設定を固定し（例えばスマホ背面カメラを想定して水平視野60°程度など）、その設定で合成データを生成しています。
	•	レンダリング: 仮想カメラから合成人物をレンダリングし、法線マップ・深度マップ・ポーズヒートマップ・セグメンテーションマップを得ます。PyTorch3DなどのDifferentiable Rendererを用いてメッシュからピクセルごとの属性を計算します。
	•	法線マップ: まずメッシュ頂点法線 $\mathcal{N}$ を各カメラの座標系に変換します（頂点法線ベクトルにカメラの回転を適用）。それを頂点カラー値としてメッシュに割り当て、ラスタライズします。ラスタライズ時はPhongシェーディングの代わりに頂点カラーをそのまま補間表示するカスタムシェーダを使い、各ピクセルのRGBがカメラ座標系法線ベクトル（0〜1正規化エンコード後の値）になるようにします。具体的には、頂点法線 $(n_x, n_y, n_z)$ を $(\frac{n_x+1}{2},\ \frac{n_y+1}{2},\ \frac{n_z+1}{2})$ に変換してRGBカラーとし、背景は黒(0,0,0)にします。出力はSapiensの法線マップと同形式（RGB各8bit, 法線成分0.5が無方向を表す）となるよう調整します。
	•	深度マップ: レンダリング時に得られるZバッファ値を利用して各ピクセルの深度（カメラ位置からの距離）を取得し、グレースケール画像にします。背景ピクセルにはデフォルト値0を割り当てます（Sapiensの深度出力も背景0の仕様）。深度値はシーン内の実スケール（メートル）ですが、そのままでは人物ごとに値域が異なるため正規化を行います。前処理と同様、人物の身長が常に一定になるようスケールします。具体的には、合成メッシュの身長（頂点Y座標のmax-min）を $H_{\text{sim}}$ [m] とし、例えば $H_{\text{sim}}=1.8$ のとき全深度値に $(1.7/1.8)$ を乗じます。つまり全被写体が仮想身長1.7mになるよう線形スケーリングします。この基準値1.7mは訓練時の統一身長であり、推論時にも用いる値です。この操作により、合成深度マップも相対距離尺度に揃い、カメラ距離によらず深度レンジが統一されます。これは実画像での深度正規化（身長基準）と対応しており、学習時にネットワークがカメラ距離の影響を簡略化して学習できる利点があります（実運用ではユーザ身長が常に与えられるため、この方式で問題ありません）。一方でネットワークに身長を入力する拡張（既に実装済み）を用いる場合、学習時にはここで敢えて高さを揃えずランダムな $H_{\text{sim}}$ で深度を生成し、その値を属性ラベルとしてネットワークに与える選択肢もあります。ただし本検証ではまず全データ統一スケールで学習させた後、微調整段階で高さ情報を導入して精度向上を図っています。
	•	ポーズヒートマップ: STARモデルのジョイントレグレッサから3D関節位置（例えばK=24箇所のジョイント座標）を取得し、それを各カメラ視点で投影して2D関節座標$(u_i, v_i)$を求めます。各関節ごとに、その位置を中心とした2Dガウシアン分布を描いたヒートマップ画像を生成します。実装上は、あらかじめガウシアンカーネルを離散化した小さなテンプレート（例:7×7）を用意し、該当画素位置に重畳させる形でチャンネルに書き込みます。これをK関節すべてに対して行い、Kチャネルのヒートマップテンソルを得ます。背景（どの関節の影響もない画素）は0のままです。なお、背面視点で前面側の関節（例: 胸や腹部正面の点）はメッシュに隠れて直接は見えませんが、ここでは両視点それぞれで全関節について投影を試みます。結果的に背面画像では前面の関節は人体シルエット外となりマスクで除去されますし、またヒートマップ自体もシルエット外は使用しないため問題ありません。むしろ、一部関節が片方の視点で見えない場合でももう片方の視点には写っているため、2ビューのヒートマップ情報を組み合わせることで完全な骨格情報が提供される利点があります。学習データ生成時には関節の投影座標は正確に得られますが、より現実的にするために後処理で微小な乱数シフトを加え、予測誤差を模擬することもできます（後述のデータ拡張参照）。
	•	セグメンテーションマップ: メッシュの各頂点に対して人体部位のカテゴリラベルを割り当てます。例えば、頭、胸部、腹部、両腕（上腕・下腕）、両脚（大腿・下腿）、手、足など適切な粒度で分類します（SMPLモデルには対応する頂点セット定義があります）。各頂点にラベルID（整数）を持たせ、PyTorch3Dのラスタライザにカスタムシェーダを組み込んで、各ピクセルにその属する三角形のラベルIDを描画します。出力としてはH×Wサイズのラベル画像（各ピクセルの値が部位ID）となります。背景ピクセルはID=0（背景クラス）で塗られます。これをそのままネットワーク入力に使うこともできますが、前述の通りOne-hotのMチャネルテンソルに変換する方法が一般的です。そこで、ラベル画像を解析してM次元のバイナリマスクテンソルに変換します（背景マスクは既に別チャネルで持っているため、ここでは背景以外の部位をM-1チャネルとしてもよいでしょう）。このようにして得た部位セグメント情報は、法線や深度では区別しにくい部位間の境界をネットワークが理解する助けとなります。
	•	背景マスク適用: レンダリングで得られた法線・深度・セグメンテーション画像には人物以外の背景部分が含まれる場合があります（ラスタライズ解像度の関係でメッシュが画角を満たさない領域など）。それをマスクします。具体的には各ピクセルについて対応するメッシュポリゴンが無ければ背景と判断し、法線は(0.5,0.5,0.5)（エンコード後の無方向）に、深度は0に、セグメンテーションはID=0に設定します。これによりSapiens出力フォーマット（背景=0）と合わせます。加えて、人物シルエットの2値マスク画像も生成して保持します（損失計算等で使用）。なお、ポーズヒートマップについては背景領域はもとより0なので追加マスク処理は不要です。
	•	データ拡張: 得られた法線・深度・ポーズ・セグメンテーションマップに対し、実データとのドメインギャップを埋めるため以下のランダム変換を適用します。
	•	法線ノイズ: 法線ベクトルについて各画素の方向をランダムに2〜5°程度揺らすノイズを加えます。実装的には法線画像のRGB値に微小なガウスノイズ（標準偏差σ=0.02程度）を加え再正規化する方法で近似します。これにより、Sapiens出力にも含まれ得る微細な予測誤差を模擬します。
	•	深度ブラー・ノイズ: 深度マップに3×3のガウシアンブラー（σ=1）で平滑化を施し、距離センサーの誤差を模擬します。また全画素に対し0.5〜1%程度のランダムな相対ノイズを加えます。さらに16bit量子化程度の丸めノイズ（スケール $10^{-3}$ の微小な段階値化）を入れることも検討しました。これらによりSapiens深度推定の不確実性を再現します。
	•	ポーズヒートマップノイズ: 各関節のヒートマップについて、中心位置を1〜3ピクセル程度ランダムにずらします。これは関節検出のわずかなずれを模擬するものです。また5〜10%の確率で特定の関節ヒートマップを消去（ゼロ化）します（片方の視点で検出が困難な関節が検出されないケースを再現）。さらにヒートマップ全体に軽微なガウシアンブラーを適用してピークをわずかにぼかし、検出確度のばらつきを表現します。
	•	セグメンテーションノイズ: 部位セグメンテーションマップについて、境界ピクセルを中心にランダムな1〜2ピクセルの膨張・収縮処理（モルフォロジー変換）を施します。これによりセグメント境界に曖昧さを加え、実際のセグメンテーション誤差を模擬します。また1〜2%の画素に対し隣接クラスにラベルを置換するノイズを入れ、所々で誤分類が起きる状況を再現します。セグメントマスクをOne-hotにする場合は、それぞれ対応するチャネルで同様の処理を行います。なお人物シルエットに関しては、背景との境界がわずかにずれる程度の変形（エッジを1ピクセル膨張など）を加えることがあります。
	•	背景ノイズ: マスク外の背景領域に、実画像風のランダム模様や色を合成します。人物には直接影響しませんが、境界ピクセル値に多様性を持たせることで境界付近の誤差に対するロバスト性を高めます。
以上のデータ拡張はPyTorchの変換（torchvision.transforms）やOpenCV処理で実装しています。なお画像全体のアフィン変換（回転・スケーリング）は適用しません。人物全体の位置や大きさを変えてしまうと、対応する真の3Dデータとの関係が崩れ、学習目標が不明瞭になるためです。
	•	入出力テンソル化: 最終的に、1つの合成サンプルから入力テンソル $\mathbf{X}$ と教師ベクトル $\mathbf{y}$ を構成します。入力 $\mathbf{X}$ は正面法線(3ch)+正面深度(1ch)+正面ポーズヒートマップ(Kch)+正面シルエット(1ch) と、背面法線(3ch)+背面深度(1ch)+背面ポーズヒートマップ(Kch)+背面シルエット(1ch) をチャネル方向にスタックした合計2*(4+K+1)チャネルのデータです。ただしネットワーク実装上は正面・背面を別入力として扱うため、ここではフロントとバックの2つのテンソルに分けて保持します（結果的にfront_input: (4+K+1)×H×W、back_input: (4+K+1)×H×W）。教師信号 $\mathbf{y}$ は対応する真のSTAR形状パラメータβ（次元 n）および真のグローバル位置 $T_{\text{gt}}$ です。ここで $T_{\text{gt}}=(T_x, T_y, T_z)$ は正面カメラ座標系での人物重心の位置で、合成データでは既知です。例えば正面カメラを $(0, \Delta_y, -D)$ に置いた場合、人物重心はカメラ座標で $(0, -\Delta_y, D)$ となるため、それを教師ベクトル $T_{\text{gt}}$ とします（背面側の位置 $T’{\text{gt}}$ は推定対象外なので使いません）。さらに、ユーザ属性をネットワークで扱う場合は、各合成データに擬似的な身長・体重・性別情報を付与し、それも教師データ（というより入力ラベル）とします。具体的には、βから計算される身長 $H{\text{sim}}$ をそのまま「擬似身長ラベル」として持たせたり、STARメッシュの体積から仮想体重（例えばBMI22相当の標準体重）を算出して「擬似体重ラベル」として持たせる方法です。また性別はあらかじめ50%の確率で男性/女性モデルを使う設定にしていればそのフラグをラベルに含めます。本実装では学習データ生成段階では性別は固定していましたが、属性入力拡張に合わせて性別もランダム化して再学習することも可能です。

以上のようにして構築されたDatasetサンプルは、後述のデータローダで逐次ネットワークに供給されます。

② ネットワーク学習

合成データによって形成された入出力ペア $(\mathbf{X}, \mathbf{y})$ を用いて形状推定ネットワークを教師あり学習します。主要な損失関数と学習設定は次の通りです。
	•	体型パラメータ損失 $L_{\beta}$: 推定βと真のβとの平均二乗誤差（MSE）です。式で書けば $L_{\beta} = \frac{1}{n}|\beta^* - \beta_{\text{gt}}|^2$ となります。β各次元は（サンプリングにより）概ね$\mathcal{N}(0,1)$に正規化されているため、単純なMSEで問題ありません。STAR形状空間の固有値に応じて次元毎に重み付けすることも可能ですが、ここでは均等に扱います。$L_{\beta}$ は本タスクの中心的目標であり、損失重み $w_{\beta}$ を大きめ（例:1.0）に設定します。
	•	位置推定損失 $L_{T}$: 推定平行移動 $T^$ と真の $T_{\text{gt}}$ の差のMSEです。すなわち $L_{T} = (T_x^ - T_{x})^2 + (T_y^* - T_{y})^2 + (T_z^* - T_{z})^2$ となります。ただし3次元それぞれのスケールが異なるため、特に奥行き方向 $T_z$ の誤差を重視します。具体的には $T_z$ 誤差に係数5〜10を掛けて損失に加算しました。これにより人物とカメラ距離の推定精度を高め、結果的にメッシュの絶対スケールが適切に推定されるよう促します。一方で $T_x, T_y$（左右・上下オフセット）は多少の誤差がβ推定に大きな影響を与えないため、そのままとします。なお学習データでは深度マップを身長基準1.7mに正規化しているため、$T_z$ の真値も「被写体身長1.7m換算でのカメラ距離」に相当します（カメラ配置距離 $D$ に対し $T_z = D \times (H_{\text{sim}}/1.7)$ で計算）。推論時にはユーザ身長で同様の補正を行っているため、学習と推論で $T_z$ のスケールは一貫しています。
	•	幾何学的一貫性損失 $L_{\text{geo}}$（オプション）: ネットワーク出力のβから再構成した3Dメッシュが入力法線・深度マップ・シルエット・2Dジョイントとどれだけ一致しているかを測る損失です。これは最終最適化工程の損失を学習時に一部取り入れる発想で、ネットワークが単に数値上βを合わせるだけでなく形状自体を合わせ込む方向に誘導できます ￼。実装としては、合成データ生成時に既に得ている教師法線・深度・シルエット・2D関節位置 ($N^{gt}, D^{gt}, S^{gt}, \mathbf{p}^{gt}$) に対し、ネットワーク出力β（および規定ポーズ $\theta$）をSTARに通して得たメッシュからレンダリングした ($N^{pred}, D^{pred}, S^{pred}, \mathbf{p}^{pred}$) を比較します。そして前述した法線損失 $L_n$・深度損失 $L_d$・シルエット損失 $L_s$・関節位置損失 $L_p$（2D投影誤差）に相当する値を計算します。ただし計算コストが大きいためデフォルトでは $L_{\text{geo}}$ は使用せず、モデルがある程度収束した後のファインチューニング段階で限定的に適用しました。
	•	総合損失関数: 上記を統合し、 $L_{\text{total}} = w_{\beta}L_{\beta} + w_T L_T + w_{\text{geo}}(L_n + L_d + L_s + L_p)$ とします（$w_{\text{geo}}$ は適用時のみ非ゼロ）。例えば初期学習では $w_{\beta}=1.0, w_T=0.1, w_{\text{geo}}=0.0$ とし、十分収束した後に $w_{\text{geo}}=0.01$ を有効化して微調整するイメージです。これによりβ推定を主目標にしつつ、距離推定と幾何整合（および関節位置整合）もわずかに考慮した学習となります。
	•	オプティマイザと学習率: 勾配最適化にはAdamを使用し、初期学習率を1e-4に設定しました。適宜コサインアニーリングやステップスケジューラで減衰（例: 5万イテレーションで1e-4→5e-5、8万で1e-5に低減）を行います。Weight Decay（L2正則化）を1e-5程度設定し汎化を促進します。混合精度学習（AMP）も有効にしてメモリ削減と演算高速化を図りました。ミニバッチサイズはGPUメモリと相談で16〜32とし、1エポックを合成データ1万サンプル相当とみなして合計10〜15エポック（=10万〜15万サンプル）学習しました。学習中は合成データを逐次生成しながらランダム供給します（全データを事前生成せずオンザフライ）。PyTorchのDataLoaderを用い、num_workers=4 程度で並行してデータ生成＋前処理を行い、GPU計算とパイプライン並列化しています。
	•	検証とモデル保存: 学習中、一定間隔で検証用データによる評価を行います。検証用には例えばSenguptaらのSSP-3Dデータセットの一部（実画像に擬似GTの3D形状ラベルが付属）を用いるなど考えられますが、本実装では簡易的に合成データの別バッチで評価しました。指標としてはウエスト周囲長の平均絶対誤差（cm）やβベクトルのMSE等を用います。エポックごとにモデルパラメータを保存し、検証誤差が最良だった回のモデルを最終モデルとして採用しました。モデルとともに学習ハイパーパラメータ（使用した $n$ や損失重み、学習率スケジュール等）も記録し、将来の再学習時に再現・比較できるようにしています。

③ 学習高速化の工夫

合成データ生成とレンダリングには計算コストが伴うため、以下の工夫で効率化しました。
	•	バッチレンダリング: PyTorch3Dのバッチレンダリング機能を活用し、ミニバッチ内の複数メッシュをまとめてレンダリングします。例えば16体分の合成データを一度にrendererに通すことで、GPU上での並列計算が可能です。これにより1体ずつ逐次処理するより高速になりました。
	•	データローダ非同期化: torch.utils.data.DataLoader を num_workers=4 で使用し、データ生成を並列スレッド/プロセスで非同期に行います。メインプロセスがGPUで学習計算中に他のワーカが次のバッチのSTAR計算やレンダリングを行い、処理待ち時間を減らします。各ワーカはプロセス開始時にSTARモデルやレンダラをロードしておき、毎回初期化するオーバーヘッドを避けています。
	•	STAR計算の効率化: STARモデル計算で繰り返し使用される定数（例: 三角形面リストやブレンドウェイト行列など）はメモリに保持し、毎回ディスクからロードしないようにしています。また、ポーズ変化が小さい設定のため線形ブレンドスキンニングの再計算コストも抑えられています。こうした工夫で1サンプルあたりの生成時間を100ms以下に収めています（マルチスレッド生成時）。
	•	AMPの活用: 学習時には torch.cuda.amp.autocast を用いて半精度（FP16）演算を行っています。特にレンダリング処理やCNNのフォワード計算で効果が大きく、メモリ消費削減とスループット向上に寄与しました。勾配適用時にはGradScalerでスケーリングすることで勾配消失や発散を防ぎつつ安定性も確保しています。

これらにより、合成データを逐次生成しつつでもGPUを高い使用率で稼働させ、現実的な時間（数日程度）で学習を完了できました。BMVC 2020のSTRAPSでは大規模合成データにより高精度を達成していますが、本システムも同様の手法で現実データに匹敵する性能を目指しています。

④ モデル更新と将来拡張

学習済みモデルは推論パイプラインに組み込み、ユーザからの画像入力に対して即座に体型推定を行います。今後、新たなデータや要件に応じてモデルを再訓練・改良することも可能です。例えば、形状パラメータ次元 $n$ を増やしてより詳細な体型表現を行う場合、設定を変更して再度合成データで学習します。その際、既存モデルの一部重み（低次元部分）を初期値として利用することで学習を安定化させることもできます。また、出力形式の拡張としてウエスト・ヒップ等の物理寸法を直接回帰するマルチタスク学習に発展させることも検討できます。実際にSmithらの研究（BfSNet）では2視点シルエットからの形状推定において既知の身長や体重を入力し、さらにメッシュ体積や関節位置を出力するマルチタスク学習で精度向上を図っています。本システムでもネットワークの全結合層出力を拡張し、βに加えて主要身体寸法（周囲長など）を予測するノードを設ければ、合成データ上で真値寸法を教師に与えて学習できます。これによりファッション用途などではユーザに直接寸法値を提示することが可能となり、利便性が向上します。

さらに、入力データや属性情報の拡充も視野に入れています。側面画像の追加やスマホ搭載LiDARの活用は既に前述しましたが、ユーザから取得できる身長・体重・性別・年齢といったメタ情報も推定精度向上に活かせます。例えば前述のBfSNetでは、人物の既知属性（性別や身長）をネットワークに入力することでシルエットからの体型推定精度を上げています。本システムでも同様に、推論時にユーザの身長・体重等を入力パラメータとして受け取り、これらをネットワーク内部で活用しています（本改訂時点ですでに実装済み）。具体的には属性値を正規化した上でCNN特徴と結合し、ネットワークが身体特徴に応じた形状を出力できるようにしました。また学習時には合成データに擬似的な体重・BMI情報を割り当て、教師信号として利用しています。たとえばβから推定されるメッシュ体積や想定BMI値を計算し、それが想定の値（仮想の「標準体重」など）になるよう損失に組み込むことで、体重情報を間接的に反映させています。性別については既にSTARモデルに男女版があるため、ユーザ性別が分かれば推論時に該当モデルを使用したり、あるいはジェンダーニュートラルモデル＋性別ワンホット入力で条件付き生成する方法も可能です。年齢も、例えば高齢になるほど姿勢が前傾する傾向を補正する等、推定結果の微調整に利用可能でしょう。現行設計ではこうした属性は前処理・後処理で利用するに留めていますが、既にネットワーク自体が属性情報を扱える拡張を取り入れたため、よりパーソナルな体型推定（例えば「50歳男性170cm80kg」に合致する3Dモデルを生成する等）も十分実現可能です。

以上、DongらのDual-view法をベースにSTARモデルとSapiensを組み合わせた本システムの方針と実装詳細を説明しました。本システムのポイントは、(1)前処理で高品質な法線・深度情報を取得し身長情報でスケール校正していること、(2)正面・背面の二視点情報に加えて2D関節位置やシルエット情報も融合し形状推定すること、(3)STARモデルの表現力豊かな形状空間を活用していること、(4)ユーザ提供の身長など属性情報を積極的に活用していることです。これらにより、入力画像からのあいまいさを低減し安定した3D体型推定が可能になります。また、既知の身長・関節位置等を適切に織り込むことで従来法より一層信頼性の高い物理寸法推定を実現できます。現在は精度重視で設計・実装していますが、今後はモデルの軽量化やリアルタイム化も視野に入れています。例えばSapiensモデルやResNetをモバイル向けに蒸留・圧縮したバージョンを使用し、スマートフォン上での動作を目指す計画もあります。その際は推論パイプラインを簡略化（1ビュー入力でも大まかに推定し、必要に応じクラウド連携で精細化など）し、ユースケースに応じたモード（高速モード vs 精密測定モード）の提供も検討されます。本仕様に基づいて実装と調整を行えば、入力画像＋身長情報＋2Dポーズ情報から安定して高精度な3D人体モデル復元が実現できる見込みです。実装完了後はいくつかの検証用データで推定精度を測定し、要件を満たしているか確認します。例えば身長・ウエスト・ヒップが既知の被験者でテストし、誤差が許容範囲（±2〜3cm以内）に収まっていることを確認します ￼。その上で、ユーザへの提供（例えばバーチャル試着サービスへの組み込み）に移行していきます。
